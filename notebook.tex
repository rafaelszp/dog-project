
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{dog\_app}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \hypertarget{artificial-intelligence-nanodegree}{%
\section{Artificial Intelligence
Nanodegree}\label{artificial-intelligence-nanodegree}}

\hypertarget{convolutional-neural-networks}{%
\subsection{Convolutional Neural
Networks}\label{convolutional-neural-networks}}

\hypertarget{project-write-an-algorithm-for-a-dog-identification-app}{%
\subsection{Project: Write an Algorithm for a Dog Identification
App}\label{project-write-an-algorithm-for-a-dog-identification-app}}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this notebook, some template code has already been provided for you,
and you will need to implement additional functionality to successfully
complete this project. You will not need to modify the included code
beyond what is requested. Sections that begin with
\textbf{`(IMPLEMENTATION)'} in the header indicate that the following
block of code will require additional functionality which you must
provide. Instructions will be provided for each section, and the
specifics of the implementation are marked in the code block with a
`TODO' statement. Please be sure to read the instructions carefully!

\begin{quote}
\textbf{Note}: Once you have completed all of the code implementations,
you need to finalize your work by exporting the iPython Notebook as an
HTML document. Before exporting the notebook to html, all of the code
cells need to have been run so that reviewers can see the final
implementation and output. You can then export the notebook by using the
menu above and navigating to \n``,''\textbf{File -\textgreater{}
Download as -\textgreater{} HTML (.html)}. Include the finished document
along with this notebook as your submission.
\end{quote}

In addition to implementing code, there will be questions that you must
answer which relate to the project and your implementation. Each section
where you will answer a question is preceded by a \textbf{`Question X'}
header. Carefully read each question and provide thorough answers in the
following text boxes that begin with \textbf{`Answer:'}. Your project
submission will be evaluated based on your answers to each of the
questions and the implementation you provide.

\begin{quote}
\textbf{Note:} Code and Markdown cells can be executed using the
\textbf{Shift + Enter} keyboard shortcut. Markdown cells can be edited
by double-clicking the cell to enter edit mode.
\end{quote}

The rubric contains \emph{optional} ``Stand Out Suggestions'' for
enhancing the project beyond the minimum requirements. If you decide to
pursue the ``Stand Out Suggestions'', you should include the code in
this IPython notebook.

 \#\# Step 0: Import Datasets

\hypertarget{import-dog-dataset}{%
\subsubsection{Import Dog Dataset}\label{import-dog-dataset}}

In the code cell below, we import a dataset of dog images. We populate a
few variables through the use of the \texttt{load\_files} function from
the scikit-learn library: - \texttt{train\_files},
\texttt{valid\_files}, \texttt{test\_files} - numpy arrays containing
file paths to images - \texttt{train\_targets}, \texttt{valid\_targets},
\texttt{test\_targets} - numpy arrays containing onehot-encoded
classification labels - \texttt{dog\_names} - list of string-valued dog
breed names for translating labels

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}76}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{datasets} \PY{k}{import} \PY{n}{load\PYZus{}files}       
         \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{utils} \PY{k}{import} \PY{n}{np\PYZus{}utils}
         \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
         \PY{k+kn}{from} \PY{n+nn}{glob} \PY{k}{import} \PY{n}{glob}
         
         \PY{c+c1}{\PYZsh{} define function to load train, test, and validation datasets}
         \PY{k}{def} \PY{n+nf}{load\PYZus{}dataset}\PY{p}{(}\PY{n}{path}\PY{p}{)}\PY{p}{:}
             \PY{n}{data} \PY{o}{=} \PY{n}{load\PYZus{}files}\PY{p}{(}\PY{n}{path}\PY{p}{)}
             \PY{n}{dog\PYZus{}files} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{filenames}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
             \PY{n}{dog\PYZus{}targets} \PY{o}{=} \PY{n}{np\PYZus{}utils}\PY{o}{.}\PY{n}{to\PYZus{}categorical}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{l+m+mi}{133}\PY{p}{)}
             \PY{k}{return} \PY{n}{dog\PYZus{}files}\PY{p}{,} \PY{n}{dog\PYZus{}targets}
         
         \PY{c+c1}{\PYZsh{} load train, test, and validation datasets}
         \PY{n}{train\PYZus{}files}\PY{p}{,} \PY{n}{train\PYZus{}targets} \PY{o}{=} \PY{n}{load\PYZus{}dataset}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dogImages/train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{valid\PYZus{}files}\PY{p}{,} \PY{n}{valid\PYZus{}targets} \PY{o}{=} \PY{n}{load\PYZus{}dataset}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dogImages/valid}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{test\PYZus{}files}\PY{p}{,} \PY{n}{test\PYZus{}targets} \PY{o}{=} \PY{n}{load\PYZus{}dataset}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dogImages/test}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} load list of dog names}
         \PY{n}{dog\PYZus{}names} \PY{o}{=} \PY{p}{[}\PY{n}{item}\PY{p}{[}\PY{l+m+mi}{20}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]} \PY{k}{for} \PY{n}{item} \PY{o+ow}{in} \PY{n+nb}{sorted}\PY{p}{(}\PY{n}{glob}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dogImages/train/*/}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{)}\PY{p}{]}
         
         \PY{c+c1}{\PYZsh{} print statistics about the dataset}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{There are }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{ total dog categories.}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n+nb}{len}\PY{p}{(}\PY{n}{dog\PYZus{}names}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{There are }\PY{l+s+si}{\PYZpc{}s}\PY{l+s+s1}{ total dog images.}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n+nb}{len}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{hstack}\PY{p}{(}\PY{p}{[}\PY{n}{train\PYZus{}files}\PY{p}{,} \PY{n}{valid\PYZus{}files}\PY{p}{,} \PY{n}{test\PYZus{}files}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{There are }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{ training dog images.}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n+nb}{len}\PY{p}{(}\PY{n}{train\PYZus{}files}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{There are }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{ validation dog images.}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n+nb}{len}\PY{p}{(}\PY{n}{valid\PYZus{}files}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{There are }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{ test dog images.}\PY{l+s+s1}{\PYZsq{}}\PY{o}{\PYZpc{}} \PY{n+nb}{len}\PY{p}{(}\PY{n}{test\PYZus{}files}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
There are 133 total dog categories.
There are 8351 total dog images.

There are 6680 training dog images.
There are 835 validation dog images.
There are 836 test dog images.

    \end{Verbatim}

    \hypertarget{import-human-dataset}{%
\subsubsection{Import Human Dataset}\label{import-human-dataset}}

In the code cell below, we import a dataset of human images, where the
file paths are stored in the numpy array \texttt{human\_files}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}77}]:} \PY{k+kn}{import} \PY{n+nn}{random}
         \PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{8675309}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} load filenames in shuffled human dataset}
         \PY{n}{human\PYZus{}files} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{glob}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{lfw/*/*}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{)}
         \PY{n}{random}\PY{o}{.}\PY{n}{shuffle}\PY{p}{(}\PY{n}{human\PYZus{}files}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} print statistics about the dataset}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{There are }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{ total human images.}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n+nb}{len}\PY{p}{(}\PY{n}{human\PYZus{}files}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
There are 13233 total human images.

    \end{Verbatim}

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

 \#\# Step 1: Detect Humans

We use OpenCV's implementation of
\href{http://docs.opencv.org/trunk/d7/d8b/tutorial_py_face_detection.html}{Haar
feature-based cascade classifiers} to detect human faces in images.
OpenCV provides many pre-trained face detectors, stored as XML files on
\href{https://github.com/opencv/opencv/tree/master/data/haarcascades}{github}.
We have downloaded one of these detectors and stored it in the
\texttt{haarcascades} directory.

In the next code cell, we demonstrate how to use this detector to find
human faces in a sample image.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}81}]:} \PY{k+kn}{import} \PY{n+nn}{cv2}                
         \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}                        
         \PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline                               
         
         \PY{c+c1}{\PYZsh{} extract pre\PYZhy{}trained face detector}
         \PY{n}{face\PYZus{}cascade} \PY{o}{=} \PY{n}{cv2}\PY{o}{.}\PY{n}{CascadeClassifier}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{haarcascades/haarcascade\PYZus{}frontalface\PYZus{}alt.xml}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} load color (BGR) image}
         \PY{n}{img} \PY{o}{=} \PY{n}{cv2}\PY{o}{.}\PY{n}{imread}\PY{p}{(}\PY{n}{human\PYZus{}files}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}
         \PY{c+c1}{\PYZsh{} convert BGR image to grayscale}
         \PY{n}{gray} \PY{o}{=} \PY{n}{cv2}\PY{o}{.}\PY{n}{cvtColor}\PY{p}{(}\PY{n}{img}\PY{p}{,} \PY{n}{cv2}\PY{o}{.}\PY{n}{COLOR\PYZus{}BGR2GRAY}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} find faces in image}
         \PY{n}{faces} \PY{o}{=} \PY{n}{face\PYZus{}cascade}\PY{o}{.}\PY{n}{detectMultiScale}\PY{p}{(}\PY{n}{gray}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} print number of faces detected in the image}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Number of faces detected:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{faces}\PY{p}{)}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} get bounding box for each detected face}
         \PY{k}{for} \PY{p}{(}\PY{n}{x}\PY{p}{,}\PY{n}{y}\PY{p}{,}\PY{n}{w}\PY{p}{,}\PY{n}{h}\PY{p}{)} \PY{o+ow}{in} \PY{n}{faces}\PY{p}{:}
             \PY{c+c1}{\PYZsh{} add bounding box to color image}
             \PY{n}{cv2}\PY{o}{.}\PY{n}{rectangle}\PY{p}{(}\PY{n}{img}\PY{p}{,}\PY{p}{(}\PY{n}{x}\PY{p}{,}\PY{n}{y}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{n}{x}\PY{o}{+}\PY{n}{w}\PY{p}{,}\PY{n}{y}\PY{o}{+}\PY{n}{h}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{l+m+mi}{255}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}
             
         \PY{c+c1}{\PYZsh{} convert BGR image to RGB for plotting}
         \PY{n}{cv\PYZus{}rgb} \PY{o}{=} \PY{n}{cv2}\PY{o}{.}\PY{n}{cvtColor}\PY{p}{(}\PY{n}{img}\PY{p}{,} \PY{n}{cv2}\PY{o}{.}\PY{n}{COLOR\PYZus{}BGR2RGB}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} display the image, along with bounding box}
         \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{cv\PYZus{}rgb}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Number of faces detected: 1

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_5_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Before using any of the face detectors, it is standard procedure to
convert the images to grayscale. The \texttt{detectMultiScale} function
executes the classifier stored in \texttt{face\_cascade} and takes the
grayscale image as a parameter.

In the above code, \texttt{faces} is a numpy array of detected faces,
where each row corresponds to a detected face. Each detected face is a
1D array with four entries that specifies the bounding box of the
detected face. The first two entries in the array (extracted in the
above code as \texttt{x} and \texttt{y}) specify the horizontal and
vertical positions of the top left corner of the bounding box. The last
two entries in the array (extracted here as \texttt{w} and \texttt{h})
specify the width and height of the box.

\hypertarget{write-a-human-face-detector}{%
\subsubsection{Write a Human Face
Detector}\label{write-a-human-face-detector}}

We can use this procedure to write a function that returns \texttt{True}
if a human face is detected in an image and \texttt{False} otherwise.
This function, aptly named \texttt{face\_detector}, takes a
string-valued file path to an image as input and appears in the code
block below.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}78}]:} \PY{c+c1}{\PYZsh{} returns \PYZdq{}True\PYZdq{} if face is detected in image stored at img\PYZus{}path}
         \PY{k}{def} \PY{n+nf}{face\PYZus{}detector}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}\PY{p}{:}
             \PY{n}{img} \PY{o}{=} \PY{n}{cv2}\PY{o}{.}\PY{n}{imread}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}
             \PY{n}{gray} \PY{o}{=} \PY{n}{cv2}\PY{o}{.}\PY{n}{cvtColor}\PY{p}{(}\PY{n}{img}\PY{p}{,} \PY{n}{cv2}\PY{o}{.}\PY{n}{COLOR\PYZus{}BGR2GRAY}\PY{p}{)}
             \PY{n}{faces} \PY{o}{=} \PY{n}{face\PYZus{}cascade}\PY{o}{.}\PY{n}{detectMultiScale}\PY{p}{(}\PY{n}{gray}\PY{p}{)}
             \PY{k}{return} \PY{n+nb}{len}\PY{p}{(}\PY{n}{faces}\PY{p}{)} \PY{o}{\PYZgt{}} \PY{l+m+mi}{0}
\end{Verbatim}


    \hypertarget{implementation-assess-the-human-face-detector}{%
\subsubsection{(IMPLEMENTATION) Assess the Human Face
Detector}\label{implementation-assess-the-human-face-detector}}

\textbf{Question 1:} Use the code cell below to test the performance of
the \texttt{face\_detector} function.\\
- What percentage of the first 100 images in \texttt{human\_files} have
a detected human face?\\
- What percentage of the first 100 images in \texttt{dog\_files} have a
detected human face?

Ideally, we would like 100\% of human images with a detected face and
0\% of dog images with a detected face. You will see that our algorithm
falls short of this goal, but still gives acceptable performance. We
extract the file paths for the first 100 images from each of the
datasets and store them in the numpy arrays \texttt{human\_files\_short}
and \texttt{dog\_files\_short}.

\textbf{Answer:}

q1.1: Detected 100\% in human\_files

q1.2: Detected 11\% in dog\_files

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}82}]:} \PY{n}{human\PYZus{}files\PYZus{}short} \PY{o}{=} \PY{n}{human\PYZus{}files}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{100}\PY{p}{]}
         \PY{n}{dog\PYZus{}files\PYZus{}short} \PY{o}{=} \PY{n}{train\PYZus{}files}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{100}\PY{p}{]}
         \PY{c+c1}{\PYZsh{} Do NOT modify the code above this line.}
         
         \PY{c+c1}{\PYZsh{}\PYZsh{} TODO: Test the performance of the face\PYZus{}detector algorithm }
         \PY{c+c1}{\PYZsh{}\PYZsh{} on the images in human\PYZus{}files\PYZus{}short and dog\PYZus{}files\PYZus{}short.}
         \PY{k}{def} \PY{n+nf}{detect\PYZus{}faces}\PY{p}{(}\PY{n}{detector}\PY{p}{)}\PY{p}{:}
             \PY{k}{global} \PY{n}{human\PYZus{}files\PYZus{}short}
             \PY{k}{global} \PY{n}{dog\PYZus{}files\PYZus{}short}
             \PY{n}{is\PYZus{}human\PYZus{}count}\PY{o}{=}\PY{l+m+mi}{0}
             \PY{n}{aint\PYZus{}human\PYZus{}count}\PY{o}{=}\PY{l+m+mi}{0}
             \PY{n}{is\PYZus{}dog\PYZus{}count}\PY{o}{=}\PY{l+m+mi}{0}
             \PY{n}{aint\PYZus{}dog\PYZus{}count}\PY{o}{=}\PY{l+m+mi}{0}
             \PY{k}{for} \PY{n}{human\PYZus{}face}\PY{p}{,}\PY{n}{dog\PYZus{}face} \PY{o+ow}{in} \PY{n+nb}{zip}\PY{p}{(}\PY{n}{human\PYZus{}files\PYZus{}short}\PY{p}{,}\PY{n}{dog\PYZus{}files\PYZus{}short}\PY{p}{)}\PY{p}{:} 
                 \PY{k}{if} \PY{n}{detector}\PY{p}{(}\PY{n}{human\PYZus{}face}\PY{p}{)}\PY{p}{:}
                     \PY{n}{is\PYZus{}human\PYZus{}count}\PY{o}{+}\PY{o}{=}\PY{l+m+mi}{1}
                 \PY{k}{else}\PY{p}{:}
                     \PY{n}{aint\PYZus{}human\PYZus{}count}\PY{o}{+}\PY{o}{=}\PY{l+m+mi}{1}
                 \PY{k}{if} \PY{o+ow}{not} \PY{n}{detector}\PY{p}{(}\PY{n}{dog\PYZus{}face}\PY{p}{)}\PY{p}{:}
                     \PY{n}{is\PYZus{}dog\PYZus{}count}\PY{o}{+}\PY{o}{=}\PY{l+m+mi}{1}
                 \PY{k}{else}\PY{p}{:}
                     \PY{n}{aint\PYZus{}dog\PYZus{}count}\PY{o}{+}\PY{o}{=}\PY{l+m+mi}{1}
             \PY{n}{total\PYZus{}count} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{human\PYZus{}files\PYZus{}short}\PY{p}{)}
             \PY{n}{human\PYZus{}percentage} \PY{o}{=} \PY{n}{is\PYZus{}human\PYZus{}count}\PY{o}{/}\PY{n}{total\PYZus{}count}
             \PY{n}{not\PYZus{}human\PYZus{}percentage} \PY{o}{=} \PY{n}{aint\PYZus{}human\PYZus{}count}\PY{o}{/}\PY{n}{total\PYZus{}count}
             \PY{n}{dog\PYZus{}percentage} \PY{o}{=} \PY{n}{is\PYZus{}dog\PYZus{}count}\PY{o}{/}\PY{n}{total\PYZus{}count}
             \PY{n}{aint\PYZus{}dog\PYZus{}percentage} \PY{o}{=} \PY{n}{aint\PYZus{}dog\PYZus{}count}\PY{o}{/}\PY{n}{total\PYZus{}count}
         
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Human percentage is }\PY{l+s+si}{\PYZob{}:.2f\PYZcb{}}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s1}{ not Human percentage is }\PY{l+s+si}{\PYZob{}:.2f\PYZcb{}}\PY{l+s+s1}{ }\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s1}{Human count}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s1}{ not Human count: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}
                   \PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{human\PYZus{}percentage}\PY{o}{*}\PY{l+m+mi}{100}\PY{p}{,}\PY{n}{not\PYZus{}human\PYZus{}percentage}\PY{p}{,}\PY{n}{is\PYZus{}human\PYZus{}count}\PY{p}{,}\PY{n}{aint\PYZus{}human\PYZus{}count}\PY{p}{)}\PY{p}{)}
         
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Dog   percentage is }\PY{l+s+si}{\PYZob{}:.2f\PYZcb{}}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s1}{ not Dog percentage is   }\PY{l+s+si}{\PYZob{}:.2f\PYZcb{}}\PY{l+s+s1}{ }\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s1}{Dog count}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s1}{ not Dog count:     }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}
                   \PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{dog\PYZus{}percentage}\PY{o}{*}\PY{l+m+mi}{100}\PY{p}{,}\PY{n}{aint\PYZus{}dog\PYZus{}percentage}\PY{o}{*}\PY{l+m+mi}{100}\PY{p}{,}\PY{n}{is\PYZus{}dog\PYZus{}count}\PY{p}{,}\PY{n}{aint\PYZus{}dog\PYZus{}count}\PY{p}{)}\PY{p}{)}
         
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Avg acc performance: }\PY{l+s+si}{\PYZob{}:.2f\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{p}{(}\PY{n}{dog\PYZus{}percentage}\PY{o}{+}\PY{n}{human\PYZus{}percentage}\PY{p}{)}\PY{o}{/}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
           
         \PY{n}{detect\PYZus{}faces}\PY{p}{(}\PY{n}{face\PYZus{}detector}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Human percentage is 100.00	 not Human percentage is 0.00 	Human count100	 not Human count: 0
Dog   percentage is 89.00	 not Dog percentage is   11.00 	Dog count89	 not Dog count:     11
Avg acc performance: 0.95

    \end{Verbatim}

    \textbf{Question 2:} This algorithmic choice necessitates that we
communicate to the user that we accept human images only when they
provide a clear view of a face (otherwise, we risk having unneccessarily
frustrated users!). In your opinion, is this a reasonable expectation to
pose on the user? If not, can you think of a way to detect humans in
images that does not necessitate an image with a clearly presented face?

\textbf{Answer:}

We suggest the face detector from OpenCV as a potential way to detect
human images in your algorithm, but you are free to explore other
approaches, especially approaches that make use of deep learning :).
Please use the code cell below to design and test your own face
detection algorithm. If you decide to pursue this \emph{optional} task,
report performance on each of the datasets.

    My answer for the question above

\emph{It depends on the case. For example, if you are building a
smartphone unlock screen feature, you'll need a clear view of a human
face, otherwise it'd cause a lot of trouble for users, without mention
the problems to the company who sold/build the smartphone. But if you
are building a security cam software with face detection, maybe would be
rather find better aproaches in order to achieve the goal.}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}83}]:} \PY{c+c1}{\PYZsh{}\PYZsh{} (Optional) TODO: Report the performance of another  }
         \PY{c+c1}{\PYZsh{}\PYZsh{} face detection algorithm on the LFW dataset}
         \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} Feel free to use as many code cells as needed.}
         \PY{k+kn}{import} \PY{n+nn}{urllib}\PY{n+nn}{.}\PY{n+nn}{request}
         \PY{k+kn}{from} \PY{n+nn}{pathlib} \PY{k}{import} \PY{n}{Path}
            
         \PY{k}{def} \PY{n+nf}{download\PYZus{}lbp\PYZus{}cc}\PY{p}{(}\PY{p}{)}\PY{p}{:}
             \PY{n}{file} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{lbpcascade\PYZus{}frontalface\PYZus{}improved.xml}\PY{l+s+s1}{\PYZsq{}}
             \PY{n}{lbp} \PY{o}{=} \PY{n}{Path}\PY{p}{(}\PY{n}{file}\PY{p}{)}
             \PY{k}{if} \PY{o+ow}{not} \PY{n}{lbp}\PY{o}{.}\PY{n}{is\PYZus{}file}\PY{p}{(}\PY{p}{)}\PY{p}{:}
                 \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Downloading LBP Cascade Classifier pre\PYZhy{}trained XML...}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                 \PY{n}{url} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{https://raw.githubusercontent.com/opencv/opencv/master/data/lbpcascades/lbpcascade\PYZus{}frontalface\PYZus{}improved.xml}\PY{l+s+s1}{\PYZsq{}}  
                 \PY{n}{urllib}\PY{o}{.}\PY{n}{request}\PY{o}{.}\PY{n}{urlretrieve}\PY{p}{(}\PY{n}{url}\PY{p}{,} \PY{n}{file}\PY{p}{)} 
             \PY{k}{else}\PY{p}{:}
                 \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{LBP Cascades were already downloaded}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          
             
         \PY{k}{def} \PY{n+nf}{lbp\PYZus{}face\PYZus{}detector}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}\PY{p}{:}
             \PY{n}{lbp\PYZus{}face\PYZus{}cascade} \PY{o}{=} \PY{n}{cv2}\PY{o}{.}\PY{n}{CascadeClassifier}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{lbpcascade\PYZus{}frontalface\PYZus{}improved.xml}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{img} \PY{o}{=} \PY{n}{cv2}\PY{o}{.}\PY{n}{imread}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}
             \PY{n}{gray} \PY{o}{=} \PY{n}{cv2}\PY{o}{.}\PY{n}{cvtColor}\PY{p}{(}\PY{n}{img}\PY{p}{,} \PY{n}{cv2}\PY{o}{.}\PY{n}{COLOR\PYZus{}BGR2GRAY}\PY{p}{)}
             \PY{n}{faces} \PY{o}{=} \PY{n}{lbp\PYZus{}face\PYZus{}cascade}\PY{o}{.}\PY{n}{detectMultiScale}\PY{p}{(}\PY{n}{gray}\PY{p}{)}
             \PY{k}{return} \PY{n+nb}{len}\PY{p}{(}\PY{n}{faces}\PY{p}{)} \PY{o}{\PYZgt{}} \PY{l+m+mi}{0}
         
         
         \PY{n}{download\PYZus{}lbp\PYZus{}cc}\PY{p}{(}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{LBP performance}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}    
         \PY{n}{detect\PYZus{}faces}\PY{p}{(}\PY{n}{lbp\PYZus{}face\PYZus{}detector}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
LBP Cascades were already downloaded
LBP performance
Human percentage is 85.00	 not Human percentage is 0.15 	Human count85	 not Human count: 15
Dog   percentage is 99.00	 not Dog percentage is   1.00 	Dog count99	 not Dog count:     1
Avg acc performance: 0.92

    \end{Verbatim}

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

 \#\# Step 2: Detect Dogs

In this section, we use a pre-trained
\href{http://ethereon.github.io/netscope/\#/gist/db945b393d40bfa26006}{ResNet-50}
model to detect dogs in images. Our first line of code downloads the
ResNet-50 model, along with weights that have been trained on
\href{http://www.image-net.org/}{ImageNet}, a very large, very popular
dataset used for image classification and other vision tasks. ImageNet
contains over 10 million URLs, each linking to an image containing an
object from one of
\href{https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a}{1000
categories}. Given an image, this pre-trained ResNet-50 model returns a
prediction (derived from the available categories in ImageNet) for the
object that is contained in the image.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}53}]:} \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{applications}\PY{n+nn}{.}\PY{n+nn}{resnet50} \PY{k}{import} \PY{n}{ResNet50}
         
         \PY{c+c1}{\PYZsh{} define ResNet50 model}
         \PY{n}{ResNet50\PYZus{}model} \PY{o}{=} \PY{n}{ResNet50}\PY{p}{(}\PY{n}{weights}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{imagenet}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \hypertarget{pre-process-the-data}{%
\subsubsection{Pre-process the Data}\label{pre-process-the-data}}

When using TensorFlow as backend, Keras CNNs require a 4D array (which
we'll also refer to as a 4D tensor) as input, with shape

\[
(\text{nb_samples}, \text{rows}, \text{columns}, \text{channels}),
\]

where \texttt{nb\_samples} corresponds to the total number of images (or
samples), and \texttt{rows}, \texttt{columns}, and \texttt{channels}
correspond to the number of rows, columns, and channels for each image,
respectively.

The \texttt{path\_to\_tensor} function below takes a string-valued file
path to a color image as input and returns a 4D tensor suitable for
supplying to a Keras CNN. The function first loads the image and resizes
it to a square image that is \(224 \times 224\) pixels. Next, the image
is converted to an array, which is then resized to a 4D tensor. In this
case, since we are working with color images, each image has three
channels. Likewise, since we are processing a single image (or sample),
the returned tensor will always have shape

\[
(1, 224, 224, 3).
\]

The \texttt{paths\_to\_tensor} function takes a numpy array of
string-valued image paths as input and returns a 4D tensor with shape

\[
(\text{nb_samples}, 224, 224, 3).
\]

Here, \texttt{nb\_samples} is the number of samples, or number of
images, in the supplied array of image paths. It is best to think of
\texttt{nb\_samples} as the number of 3D tensors (where each 3D tensor
corresponds to a different image) in your dataset!

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}54}]:} \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{preprocessing} \PY{k}{import} \PY{n}{image}                  
         \PY{k+kn}{from} \PY{n+nn}{tqdm} \PY{k}{import} \PY{n}{tqdm}
         
         \PY{k}{def} \PY{n+nf}{path\PYZus{}to\PYZus{}tensor}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}\PY{p}{:}
             \PY{c+c1}{\PYZsh{} loads RGB image as PIL.Image.Image type}
             \PY{n}{img} \PY{o}{=} \PY{n}{image}\PY{o}{.}\PY{n}{load\PYZus{}img}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{,} \PY{n}{target\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{224}\PY{p}{,} \PY{l+m+mi}{224}\PY{p}{)}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)}
             \PY{n}{x} \PY{o}{=} \PY{n}{image}\PY{o}{.}\PY{n}{img\PYZus{}to\PYZus{}array}\PY{p}{(}\PY{n}{img}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor}
             \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{expand\PYZus{}dims}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
         
         \PY{k}{def} \PY{n+nf}{paths\PYZus{}to\PYZus{}tensor}\PY{p}{(}\PY{n}{img\PYZus{}paths}\PY{p}{)}\PY{p}{:}
             \PY{n}{list\PYZus{}of\PYZus{}tensors} \PY{o}{=} \PY{p}{[}\PY{n}{path\PYZus{}to\PYZus{}tensor}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)} \PY{k}{for} \PY{n}{img\PYZus{}path} \PY{o+ow}{in} \PY{n}{tqdm}\PY{p}{(}\PY{n}{img\PYZus{}paths}\PY{p}{)}\PY{p}{]}
             \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{vstack}\PY{p}{(}\PY{n}{list\PYZus{}of\PYZus{}tensors}\PY{p}{)}
\end{Verbatim}


    \hypertarget{making-predictions-with-resnet-50}{%
\subsubsection{Making Predictions with
ResNet-50}\label{making-predictions-with-resnet-50}}

Getting the 4D tensor ready for ResNet-50, and for any other pre-trained
model in Keras, requires some additional processing. First, the RGB
image is converted to BGR by reordering the channels. All pre-trained
models have the additional normalization step that the mean pixel
(expressed in RGB as \([103.939, 116.779, 123.68]\) and calculated from
all pixels in all images in ImageNet) must be subtracted from every
pixel in each image. This is implemented in the imported function
\texttt{preprocess\_input}. If you're curious, you can check the code
for \texttt{preprocess\_input}
\href{https://github.com/fchollet/keras/blob/master/keras/applications/imagenet_utils.py}{here}.

Now that we have a way to format our image for supplying to ResNet-50,
we are now ready to use the model to extract the predictions. This is
accomplished with the \texttt{predict} method, which returns an array
whose \(i\)-th entry is the model's predicted probability that the image
belongs to the \(i\)-th ImageNet category. This is implemented in the
\texttt{ResNet50\_predict\_labels} function below.

By taking the argmax of the predicted probability vector, we obtain an
integer corresponding to the model's predicted object class, which we
can identify with an object category through the use of this
\href{https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a}{dictionary}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}57}]:} \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{applications}\PY{n+nn}{.}\PY{n+nn}{resnet50} \PY{k}{import} \PY{n}{preprocess\PYZus{}input}\PY{p}{,} \PY{n}{decode\PYZus{}predictions}
         
         \PY{k}{def} \PY{n+nf}{ResNet50\PYZus{}predict\PYZus{}labels}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}\PY{p}{:}
             \PY{c+c1}{\PYZsh{} returns prediction vector for image located at img\PYZus{}path}
             \PY{n}{img} \PY{o}{=} \PY{n}{preprocess\PYZus{}input}\PY{p}{(}\PY{n}{path\PYZus{}to\PYZus{}tensor}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}\PY{p}{)}
             \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{ResNet50\PYZus{}model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{img}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \hypertarget{write-a-dog-detector}{%
\subsubsection{Write a Dog Detector}\label{write-a-dog-detector}}

While looking at the
\href{https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a}{dictionary},
you will notice that the categories corresponding to dogs appear in an
uninterrupted sequence and correspond to dictionary keys 151-268,
inclusive, to include all categories from
\texttt{\textquotesingle{}Chihuahua\textquotesingle{}} to
\texttt{\textquotesingle{}Mexican\ hairless\textquotesingle{}}. Thus, in
order to check to see if an image is predicted to contain a dog by the
pre-trained ResNet-50 model, we need only check if the
\texttt{ResNet50\_predict\_labels} function above returns a value
between 151 and 268 (inclusive).

We use these ideas to complete the \texttt{dog\_detector} function
below, which returns \texttt{True} if a dog is detected in an image (and
\texttt{False} if not).

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}102}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} returns \PYZdq{}True\PYZdq{} if a dog is detected in the image stored at img\PYZus{}path}
          \PY{k}{def} \PY{n+nf}{dog\PYZus{}detector}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}\PY{p}{:}
              \PY{n}{prediction} \PY{o}{=} \PY{n}{ResNet50\PYZus{}predict\PYZus{}labels}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}
              \PY{k}{return} \PY{p}{(}\PY{p}{(}\PY{n}{prediction} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{l+m+mi}{268}\PY{p}{)} \PY{o}{\PYZam{}} \PY{p}{(}\PY{n}{prediction} \PY{o}{\PYZgt{}}\PY{o}{=} \PY{l+m+mi}{151}\PY{p}{)}\PY{p}{)} 
\end{Verbatim}


    \hypertarget{implementation-assess-the-dog-detector}{%
\subsubsection{(IMPLEMENTATION) Assess the Dog
Detector}\label{implementation-assess-the-dog-detector}}

\textbf{Question 3:} Use the code cell below to test the performance of
your \texttt{dog\_detector} function.\\
- What percentage of the images in \texttt{human\_files\_short} have a
detected dog?\\
- What percentage of the images in \texttt{dog\_files\_short} have a
detected dog?

\textbf{Answer:}

\textbf{q3.1: } 0\%. That's amazing!

\textbf{q3.2: } 100\%. Amazing again!

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}103}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} TODO: Test the performance of the dog\PYZus{}detector function}
          \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} on the images in human\PYZus{}files\PYZus{}short and dog\PYZus{}files\PYZus{}short.}
          \PY{n}{human\PYZus{}files\PYZus{}short} \PY{o}{=} \PY{n}{human\PYZus{}files}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{100}\PY{p}{]}
          \PY{n}{dog\PYZus{}files\PYZus{}short} \PY{o}{=} \PY{n}{train\PYZus{}files}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{100}\PY{p}{]}
          \PY{n}{total\PYZus{}count} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{human\PYZus{}files\PYZus{}short}\PY{p}{)}
          \PY{n}{detected\PYZus{}dogs\PYZus{}in\PYZus{}human\PYZus{}files}\PY{o}{=}\PY{l+m+mi}{0}
          \PY{n}{detected\PYZus{}dogs\PYZus{}in\PYZus{}dog\PYZus{}files}\PY{o}{=}\PY{l+m+mi}{0}
          \PY{k}{for} \PY{n}{human\PYZus{}file}\PY{p}{,} \PY{n}{dog\PYZus{}file} \PY{o+ow}{in} \PY{n+nb}{zip}\PY{p}{(}\PY{n}{human\PYZus{}files\PYZus{}short}\PY{p}{,}\PY{n}{dog\PYZus{}files\PYZus{}short}\PY{p}{)}\PY{p}{:}
              \PY{n}{detected\PYZus{}dogs\PYZus{}in\PYZus{}human\PYZus{}files} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1} \PY{k}{if} \PY{n}{dog\PYZus{}detector}\PY{p}{(}\PY{n}{human\PYZus{}file}\PY{p}{)}\PY{o}{==}\PY{k+kc}{True} \PY{k}{else} \PY{l+m+mi}{0}
              \PY{n}{detected\PYZus{}dogs\PYZus{}in\PYZus{}dog\PYZus{}files} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1} \PY{k}{if} \PY{n}{dog\PYZus{}detector}\PY{p}{(}\PY{n}{dog\PYZus{}file}\PY{p}{)}\PY{o}{==}\PY{k+kc}{True} \PY{k}{else} \PY{l+m+mi}{0}
          
          \PY{n}{detected\PYZus{}dogs\PYZus{}in\PYZus{}human\PYZus{}files\PYZus{}percentage} \PY{o}{=} \PY{n}{detected\PYZus{}dogs\PYZus{}in\PYZus{}human\PYZus{}files}\PY{o}{/}\PY{n}{total\PYZus{}count}
          \PY{n}{detected\PYZus{}dogs\PYZus{}in\PYZus{}dog\PYZus{}files\PYZus{}percentage} \PY{o}{=} \PY{n}{detected\PYZus{}dogs\PYZus{}in\PYZus{}dog\PYZus{}files}\PY{o}{/}\PY{n}{total\PYZus{}count}
          
          \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Dogs Percentage of the images in human\PYZus{}files\PYZus{}short: }\PY{l+s+se}{\PYZbs{}t}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+si}{\PYZob{}:.2f\PYZcb{}}\PY{l+s+s2}{ }\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{detected\PYZus{}dogs\PYZus{}in\PYZus{}human\PYZus{}files\PYZus{}percentage}\PY{o}{*}\PY{l+m+mi}{100}\PY{p}{)}\PY{p}{)}
          \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Dogs Percentage of the images in dog\PYZus{}files\PYZus{}short: }\PY{l+s+se}{\PYZbs{}t}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+si}{\PYZob{}:.2f\PYZcb{}}\PY{l+s+s2}{ }\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{detected\PYZus{}dogs\PYZus{}in\PYZus{}dog\PYZus{}files\PYZus{}percentage}\PY{o}{*}\PY{l+m+mi}{100}\PY{p}{)}\PY{p}{)}
              
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Dogs Percentage of the images in human\_files\_short: 		0.00 
Dogs Percentage of the images in dog\_files\_short: 		100.00 

    \end{Verbatim}

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

 \#\# Step 3: Create a CNN to Classify Dog Breeds (from Scratch)

Now that we have functions for detecting humans and dogs in images, we
need a way to predict breed from images. In this step, you will create a
CNN that classifies dog breeds. You must create your CNN \emph{from
scratch} (so, you can't use transfer learning \emph{yet}!), and you must
attain a test accuracy of at least 1\%. In Step 5 of this notebook, you
will have the opportunity to use transfer learning to create a CNN that
attains greatly improved accuracy.

Be careful with adding too many trainable layers! More parameters means
longer training, which means you are more likely to need a GPU to
accelerate the training process. Thankfully, Keras provides a handy
estimate of the time that each epoch is likely to take; you can
extrapolate this estimate to figure out how long it will take for your
algorithm to train.

We mention that the task of assigning breed to dogs from images is
considered exceptionally challenging. To see why, consider that
\emph{even a human} would have great difficulty in distinguishing
between a Brittany and a Welsh Springer Spaniel.

\begin{longtable}[]{@{}ll@{}}
\toprule
Brittany & Welsh Springer Spaniel\tabularnewline
\midrule
\endhead
&\tabularnewline
\bottomrule
\end{longtable}

It is not difficult to find other dog breed pairs with minimal
inter-class variation (for instance, Curly-Coated Retrievers and
American Water Spaniels).

\begin{longtable}[]{@{}ll@{}}
\toprule
Curly-Coated Retriever & American Water Spaniel\tabularnewline
\midrule
\endhead
&\tabularnewline
\bottomrule
\end{longtable}

Likewise, recall that labradors come in yellow, chocolate, and black.
Your vision-based algorithm will have to conquer this high intra-class
variation to determine how to classify all of these different shades as
the same breed.

\begin{longtable}[]{@{}ll@{}}
\toprule
Yellow Labrador & Chocolate Labrador\tabularnewline
\midrule
\endhead
&\tabularnewline
\bottomrule
\end{longtable}

We also mention that random chance presents an exceptionally low bar:
setting aside the fact that the classes are slightly imabalanced, a
random guess will provide a correct answer roughly 1 in 133 times, which
corresponds to an accuracy of less than 1\%.

Remember that the practice is far ahead of the theory in deep learning.
Experiment with many different architectures, and trust your intuition.
And, of course, have fun!

\hypertarget{pre-process-the-data}{%
\subsubsection{Pre-process the Data}\label{pre-process-the-data}}

We rescale the images by dividing every pixel in every image by 255.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{k+kn}{from} \PY{n+nn}{PIL} \PY{k}{import} \PY{n}{ImageFile}                            
        \PY{n}{ImageFile}\PY{o}{.}\PY{n}{LOAD\PYZus{}TRUNCATED\PYZus{}IMAGES} \PY{o}{=} \PY{k+kc}{True}                 
        
        \PY{c+c1}{\PYZsh{}train\PYZus{}files = train\PYZus{}files[:3000]}
        \PY{c+c1}{\PYZsh{}train\PYZus{}targets = train\PYZus{}targets[:3000]}
        
        \PY{c+c1}{\PYZsh{} pre\PYZhy{}process the data for Keras}
        \PY{n}{train\PYZus{}tensors} \PY{o}{=} \PY{n}{paths\PYZus{}to\PYZus{}tensor}\PY{p}{(}\PY{n}{train\PYZus{}files}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{float32}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{/}\PY{l+m+mi}{255}
        \PY{n}{valid\PYZus{}tensors} \PY{o}{=} \PY{n}{paths\PYZus{}to\PYZus{}tensor}\PY{p}{(}\PY{n}{valid\PYZus{}files}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{float32}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{/}\PY{l+m+mi}{255}
        \PY{n}{test\PYZus{}tensors} \PY{o}{=} \PY{n}{paths\PYZus{}to\PYZus{}tensor}\PY{p}{(}\PY{n}{test\PYZus{}files}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{float32}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{/}\PY{l+m+mi}{255}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
100\%|██████████| 6680/6680 [01:19<00:00, 84.44it/s]
100\%|██████████| 835/835 [00:08<00:00, 95.41it/s] 
100\%|██████████| 836/836 [00:08<00:00, 94.22it/s] 

    \end{Verbatim}

    \hypertarget{implementation-model-architecture}{%
\subsubsection{(IMPLEMENTATION) Model
Architecture}\label{implementation-model-architecture}}

Create a CNN to classify dog breed. At the end of your code cell block,
summarize the layers of your model by executing the line:

\begin{verbatim}
    model.summary()
\end{verbatim}

We have imported some Python modules to get you started, but feel free
to import as many modules as you need. If you end up getting stuck,
here's a hint that specifies a model that trains relatively fast on CPU
and attains \textgreater{}1\% test accuracy in 5 epochs:

\begin{figure}
\centering
\includegraphics{images/sample_cnn.png}
\caption{Sample CNN}
\end{figure}

\textbf{Question 4:} Outline the steps you took to get to your final CNN
architecture and your reasoning at each step. If you chose to use the
hinted architecture above, describe why you think that CNN architecture
should work well for the image classification task.

\textbf{Answer:} It was a quite demanding hard work until I reach this
reasonable architecture. I think I could stack 2 or 3 more layers, but
as as long as I'm running in a CPU it would take ages to finish
processing. At first, I tried to stack conv layers using
\textbf{Conv-\textgreater{}MaxPool-\textgreater{}Dropout\emph{n-\textgreater{}fc1-\textgreater{}fc2\textbf{,
but I my model couldn't reach the 1\% threshold. So I tried some other
archictectures like below: 1.
https://www.learnopencv.com/image-classification-using-convolutional-neural-networks-in-keras/,
item 3.2 This model wasn't very accurate for this type of problem. Gave
me only 1\% accuracy 2. https://github.com/abgoswam/deepdish, Deep
Learning for Classifying Food Dishes. I tried an adaptation to this
model which its very expensive for my CPU. I tried to stack
}3*Convs(kern=5x5)-\textgreater{}2}Convs(kern5x5,stride=2)-\textgreater{}GAP-\textgreater{}FC}.
But I ran out of resources, getting python being killed. So I tried to
train with only 1000 samples, with 20 epochs, took me 4 hours to give me
a 3\% accuracy. So I gave up this model because I really need a GPU to
evaluate and fine tune this model.

In order to reach this final architecture I took the suggested arch as
base for my own, I searched a little bit about how to get better results
and I found out that I could use BatchNormalization. This approach was a
game changer because when running with 10 epochs my model got up to 5\%
of accuracy. So I increased the number of epochs and aplied data
augumentation which gave me 23\% of accuracy. I think if I use another
approaches I can get better results, but my lack of resources are a
barrier for me so far.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{layers} \PY{k}{import} \PY{n}{Conv2D}\PY{p}{,} \PY{n}{MaxPooling2D}\PY{p}{,} \PY{n}{GlobalAveragePooling2D}\PY{p}{,}\PY{n}{UpSampling2D}
        \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{layers} \PY{k}{import} \PY{n}{Dropout}\PY{p}{,} \PY{n}{Flatten}\PY{p}{,} \PY{n}{Dense}
        \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{models} \PY{k}{import} \PY{n}{Sequential}
        \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{layers}\PY{n+nn}{.}\PY{n+nn}{normalization} \PY{k}{import} \PY{n}{BatchNormalization}
        
        
        \PY{n}{model} \PY{o}{=} \PY{n}{Sequential}\PY{p}{(}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} TODO: Define your architecture.}
        \PY{k}{def} \PY{n+nf}{add\PYZus{}conv}\PY{p}{(}\PY{n}{model}\PY{p}{,}\PY{n}{n\PYZus{}filter}\PY{p}{,}\PY{n}{kernel\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,}\PY{n}{shape}\PY{o}{=}\PY{k+kc}{None}\PY{p}{)}\PY{p}{:}
            \PY{k}{if} \PY{o+ow}{not} \PY{n}{shape}\PY{p}{:}
                \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{BatchNormalization}\PY{p}{(}\PY{p}{)}\PY{p}{)}
                \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv2D}\PY{p}{(}\PY{n}{n\PYZus{}filter}\PY{p}{,}\PY{n}{kernel\PYZus{}size}\PY{p}{,}\PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{kernel\PYZus{}initializer}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{TruncatedNormal}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
            \PY{k}{else}\PY{p}{:}
                \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{BatchNormalization}\PY{p}{(}\PY{n}{input\PYZus{}shape}\PY{o}{=}\PY{n}{shape}\PY{p}{)}\PY{p}{)}
                \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv2D}\PY{p}{(}\PY{n}{n\PYZus{}filter}\PY{p}{,}\PY{n}{kernel\PYZus{}size}\PY{p}{,}\PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{input\PYZus{}shape}\PY{o}{=}\PY{n}{shape}\PY{p}{,}\PY{n}{kernel\PYZus{}initializer}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{TruncatedNormal}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
            \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling2D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{p}{)}
            \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.2}\PY{p}{)}\PY{p}{)}
            
            
        
        \PY{n}{num\PYZus{}classes} \PY{o}{=} \PY{l+m+mi}{133}
        \PY{n}{input\PYZus{}shape}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{224}\PY{p}{,}\PY{l+m+mi}{224}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}
        \PY{n}{add\PYZus{}conv}\PY{p}{(}\PY{n}{model}\PY{p}{,}\PY{l+m+mi}{16}\PY{p}{,}\PY{n}{shape}\PY{o}{=}\PY{n}{input\PYZus{}shape}\PY{p}{)}
        \PY{n}{add\PYZus{}conv}\PY{p}{(}\PY{n}{model}\PY{p}{,}\PY{l+m+mi}{32}\PY{p}{)}
        \PY{n}{add\PYZus{}conv}\PY{p}{(}\PY{n}{model}\PY{p}{,}\PY{l+m+mi}{64}\PY{p}{)}
        \PY{n}{add\PYZus{}conv}\PY{p}{(}\PY{n}{model}\PY{p}{,}\PY{l+m+mi}{128}\PY{p}{)}
        \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{BatchNormalization}\PY{p}{(}\PY{p}{)}\PY{p}{)}
        
        \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{GlobalAveragePooling2D}\PY{p}{(}\PY{p}{)}\PY{p}{)}
        \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{n}{num\PYZus{}classes}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{softmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
        
        \PY{n}{model}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                 Output Shape              Param \#   
=================================================================
batch\_normalization\_1 (Batch (None, 224, 224, 3)       12        
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
conv2d\_1 (Conv2D)            (None, 224, 224, 16)      448       
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling2d\_1 (MaxPooling2 (None, 112, 112, 16)      0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dropout\_1 (Dropout)          (None, 112, 112, 16)      0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
batch\_normalization\_2 (Batch (None, 112, 112, 16)      64        
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
conv2d\_2 (Conv2D)            (None, 112, 112, 32)      4640      
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling2d\_2 (MaxPooling2 (None, 56, 56, 32)        0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dropout\_2 (Dropout)          (None, 56, 56, 32)        0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
batch\_normalization\_3 (Batch (None, 56, 56, 32)        128       
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
conv2d\_3 (Conv2D)            (None, 56, 56, 64)        18496     
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling2d\_3 (MaxPooling2 (None, 28, 28, 64)        0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dropout\_3 (Dropout)          (None, 28, 28, 64)        0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
batch\_normalization\_4 (Batch (None, 28, 28, 64)        256       
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
conv2d\_4 (Conv2D)            (None, 28, 28, 128)       73856     
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling2d\_4 (MaxPooling2 (None, 14, 14, 128)       0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dropout\_4 (Dropout)          (None, 14, 14, 128)       0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
batch\_normalization\_5 (Batch (None, 14, 14, 128)       512       
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
global\_average\_pooling2d\_1 ( (None, 128)               0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_1 (Dense)              (None, 133)               17157     
=================================================================
Total params: 115,569.0
Trainable params: 115,083.0
Non-trainable params: 486.0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

    \end{Verbatim}

    \hypertarget{compile-the-model}{%
\subsubsection{Compile the Model}\label{compile-the-model}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{n}{model}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{optimizer}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{rmsprop}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{categorical\PYZus{}crossentropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \hypertarget{implementation-train-the-model}{%
\subsubsection{(IMPLEMENTATION) Train the
Model}\label{implementation-train-the-model}}

Train your model in the code cell below. Use model checkpointing to save
the model that attains the best validation loss.

You are welcome to
\href{https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html}{augment
the training data}, but this is not a requirement.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{callbacks} \PY{k}{import} \PY{n}{ModelCheckpoint}  
        \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{preprocessing}\PY{n+nn}{.}\PY{n+nn}{image} \PY{k}{import} \PY{n}{ImageDataGenerator}
        
        \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} TODO: specify the number of epochs that you would like to use to train the model.}
        
        \PY{n}{epochs} \PY{o}{=} \PY{l+m+mi}{100}
        \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{20}
        
        \PY{n}{augumented\PYZus{}dataset} \PY{o}{=} \PY{n}{ImageDataGenerator}\PY{p}{(}
                \PY{n}{rotation\PYZus{}range}\PY{o}{=}\PY{l+m+mi}{40}\PY{p}{,}
                \PY{n}{width\PYZus{}shift\PYZus{}range}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,}
                \PY{n}{height\PYZus{}shift\PYZus{}range}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,}
                \PY{n}{shear\PYZus{}range}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,}
                \PY{n}{zoom\PYZus{}range}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,}
                \PY{n}{horizontal\PYZus{}flip}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}
                \PY{n}{fill\PYZus{}mode}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{nearest}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{augumented\PYZus{}dataset}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{train\PYZus{}tensors}\PY{p}{)}
        
        
        
        \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} Do NOT modify the code below this line.}
        
        \PY{n}{checkpointer} \PY{o}{=} \PY{n}{ModelCheckpoint}\PY{p}{(}\PY{n}{filepath}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{saved\PYZus{}models/weights.best.from\PYZus{}scratch.hdf5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                                       \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{save\PYZus{}best\PYZus{}only}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{}model.fit(train\PYZus{}tensors, train\PYZus{}targets, }
        \PY{c+c1}{\PYZsh{}          validation\PYZus{}data=(valid\PYZus{}tensors, valid\PYZus{}targets),}
        \PY{c+c1}{\PYZsh{}          epochs=epochs, batch\PYZus{}size=20, callbacks=[checkpointer], verbose=1)}
        \PY{n}{model}\PY{o}{.}\PY{n}{fit\PYZus{}generator}\PY{p}{(}\PY{n}{augumented\PYZus{}dataset}\PY{o}{.}\PY{n}{flow}\PY{p}{(}\PY{n}{train\PYZus{}tensors}\PY{p}{,} \PY{n}{train\PYZus{}targets}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{n}{batch\PYZus{}size}\PY{p}{)}\PY{p}{,}
                            \PY{n}{validation\PYZus{}data}\PY{o}{=}\PY{p}{(}\PY{n}{valid\PYZus{}tensors}\PY{p}{,} \PY{n}{valid\PYZus{}targets}\PY{p}{)}\PY{p}{,} 
                            \PY{n}{steps\PYZus{}per\PYZus{}epoch}\PY{o}{=}\PY{n}{train\PYZus{}tensors}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{/}\PY{o}{/} \PY{n}{batch\PYZus{}size}\PY{p}{,}
                            \PY{n}{epochs}\PY{o}{=}\PY{n}{epochs}\PY{p}{,} \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{checkpointer}\PY{p}{]}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Epoch 1/100
333/334 [============================>.] - ETA: 1s - loss: 4.3374 - acc: 0.0697Epoch 00000: val\_loss improved from inf to 4.41998, saving model to saved\_models/weights.best.from\_scratch.hdf5
334/334 [==============================] - 423s - loss: 4.3373 - acc: 0.0698 - val\_loss: 4.4200 - val\_acc: 0.0587
Epoch 2/100
333/334 [============================>.] - ETA: 1s - loss: 4.2752 - acc: 0.0830Epoch 00001: val\_loss improved from 4.41998 to 4.31052, saving model to saved\_models/weights.best.from\_scratch.hdf5
334/334 [==============================] - 414s - loss: 4.2753 - acc: 0.0832 - val\_loss: 4.3105 - val\_acc: 0.0886
Epoch 3/100
333/334 [============================>.] - ETA: 1s - loss: 4.2328 - acc: 0.0832Epoch 00002: val\_loss did not improve
334/334 [==============================] - 414s - loss: 4.2336 - acc: 0.0829 - val\_loss: 4.3156 - val\_acc: 0.0731
Epoch 4/100
333/334 [============================>.] - ETA: 1s - loss: 4.1679 - acc: 0.0995Epoch 00003: val\_loss improved from 4.31052 to 4.25167, saving model to saved\_models/weights.best.from\_scratch.hdf5
334/334 [==============================] - 413s - loss: 4.1680 - acc: 0.0994 - val\_loss: 4.2517 - val\_acc: 0.0754
Epoch 5/100
333/334 [============================>.] - ETA: 1s - loss: 4.1176 - acc: 0.1014Epoch 00004: val\_loss improved from 4.25167 to 4.18284, saving model to saved\_models/weights.best.from\_scratch.hdf5
334/334 [==============================] - 417s - loss: 4.1174 - acc: 0.1015 - val\_loss: 4.1828 - val\_acc: 0.0934
Epoch 6/100
333/334 [============================>.] - ETA: 1s - loss: 4.0632 - acc: 0.1101Epoch 00005: val\_loss did not improve
334/334 [==============================] - 411s - loss: 4.0622 - acc: 0.1100 - val\_loss: 4.2168 - val\_acc: 0.0898
Epoch 7/100
333/334 [============================>.] - ETA: 1s - loss: 4.0080 - acc: 0.1131Epoch 00006: val\_loss improved from 4.18284 to 4.11605, saving model to saved\_models/weights.best.from\_scratch.hdf5
334/334 [==============================] - 411s - loss: 4.0073 - acc: 0.1133 - val\_loss: 4.1160 - val\_acc: 0.1066
Epoch 8/100
333/334 [============================>.] - ETA: 1s - loss: 3.9461 - acc: 0.1276Epoch 00007: val\_loss improved from 4.11605 to 4.05692, saving model to saved\_models/weights.best.from\_scratch.hdf5
334/334 [==============================] - 406s - loss: 3.9452 - acc: 0.1280 - val\_loss: 4.0569 - val\_acc: 0.0982
Epoch 9/100
333/334 [============================>.] - ETA: 1s - loss: 3.9195 - acc: 0.1335Epoch 00008: val\_loss did not improve
334/334 [==============================] - 416s - loss: 3.9202 - acc: 0.1332 - val\_loss: 4.0840 - val\_acc: 0.0910
Epoch 10/100
333/334 [============================>.] - ETA: 1s - loss: 3.8819 - acc: 0.1362Epoch 00009: val\_loss improved from 4.05692 to 3.94825, saving model to saved\_models/weights.best.from\_scratch.hdf5
334/334 [==============================] - 412s - loss: 3.8827 - acc: 0.1361 - val\_loss: 3.9483 - val\_acc: 0.1269
Epoch 11/100
333/334 [============================>.] - ETA: 1s - loss: 3.8392 - acc: 0.1422Epoch 00010: val\_loss did not improve
334/334 [==============================] - 411s - loss: 3.8397 - acc: 0.1421 - val\_loss: 4.0771 - val\_acc: 0.1018
Epoch 12/100
333/334 [============================>.] - ETA: 1s - loss: 3.7982 - acc: 0.1544Epoch 00011: val\_loss did not improve
334/334 [==============================] - 411s - loss: 3.7988 - acc: 0.1540 - val\_loss: 4.1142 - val\_acc: 0.0886
Epoch 13/100
333/334 [============================>.] - ETA: 1s - loss: 3.7655 - acc: 0.1602Epoch 00012: val\_loss did not improve
334/334 [==============================] - 418s - loss: 3.7653 - acc: 0.1606 - val\_loss: 3.9720 - val\_acc: 0.1222
Epoch 14/100
333/334 [============================>.] - ETA: 1s - loss: 3.7364 - acc: 0.1569Epoch 00013: val\_loss improved from 3.94825 to 3.86983, saving model to saved\_models/weights.best.from\_scratch.hdf5
334/334 [==============================] - 420s - loss: 3.7364 - acc: 0.1569 - val\_loss: 3.8698 - val\_acc: 0.1269
Epoch 15/100
333/334 [============================>.] - ETA: 1s - loss: 3.6995 - acc: 0.1674Epoch 00014: val\_loss did not improve
334/334 [==============================] - 406s - loss: 3.6994 - acc: 0.1677 - val\_loss: 4.0850 - val\_acc: 0.1114
Epoch 16/100
333/334 [============================>.] - ETA: 1s - loss: 3.6653 - acc: 0.1734Epoch 00015: val\_loss did not improve
334/334 [==============================] - 410s - loss: 3.6655 - acc: 0.1732 - val\_loss: 4.0245 - val\_acc: 0.1269
Epoch 17/100
333/334 [============================>.] - ETA: 1s - loss: 3.6563 - acc: 0.1692Epoch 00016: val\_loss improved from 3.86983 to 3.80394, saving model to saved\_models/weights.best.from\_scratch.hdf5
334/334 [==============================] - 404s - loss: 3.6556 - acc: 0.1693 - val\_loss: 3.8039 - val\_acc: 0.1497
Epoch 18/100
333/334 [============================>.] - ETA: 1s - loss: 3.6218 - acc: 0.1862Epoch 00017: val\_loss did not improve
334/334 [==============================] - 417s - loss: 3.6214 - acc: 0.1864 - val\_loss: 3.8958 - val\_acc: 0.1174
Epoch 19/100
333/334 [============================>.] - ETA: 1s - loss: 3.6044 - acc: 0.1943Epoch 00018: val\_loss did not improve
334/334 [==============================] - 411s - loss: 3.6047 - acc: 0.1942 - val\_loss: 3.8445 - val\_acc: 0.1389
Epoch 20/100
333/334 [============================>.] - ETA: 1s - loss: 3.5717 - acc: 0.1886Epoch 00019: val\_loss improved from 3.80394 to 3.71716, saving model to saved\_models/weights.best.from\_scratch.hdf5
334/334 [==============================] - 412s - loss: 3.5709 - acc: 0.1885 - val\_loss: 3.7172 - val\_acc: 0.1617
Epoch 21/100
333/334 [============================>.] - ETA: 1s - loss: 3.5343 - acc: 0.1952Epoch 00020: val\_loss did not improve
334/334 [==============================] - 406s - loss: 3.5352 - acc: 0.1949 - val\_loss: 3.8656 - val\_acc: 0.1269
Epoch 22/100
333/334 [============================>.] - ETA: 1s - loss: 3.5047 - acc: 0.2002Epoch 00021: val\_loss did not improve
334/334 [==============================] - 414s - loss: 3.5056 - acc: 0.1999 - val\_loss: 3.7926 - val\_acc: 0.1401
Epoch 23/100
333/334 [============================>.] - ETA: 1s - loss: 3.4955 - acc: 0.2036Epoch 00022: val\_loss did not improve
334/334 [==============================] - 405s - loss: 3.4946 - acc: 0.2037 - val\_loss: 3.9080 - val\_acc: 0.1377
Epoch 24/100
333/334 [============================>.] - ETA: 1s - loss: 3.4840 - acc: 0.2014Epoch 00023: val\_loss improved from 3.71716 to 3.68378, saving model to saved\_models/weights.best.from\_scratch.hdf5
334/334 [==============================] - 404s - loss: 3.4831 - acc: 0.2018 - val\_loss: 3.6838 - val\_acc: 0.1557
Epoch 25/100
333/334 [============================>.] - ETA: 1s - loss: 3.4401 - acc: 0.2113Epoch 00024: val\_loss did not improve
334/334 [==============================] - 405s - loss: 3.4425 - acc: 0.2109 - val\_loss: 4.2995 - val\_acc: 0.0922
Epoch 26/100
333/334 [============================>.] - ETA: 1s - loss: 3.4431 - acc: 0.2156Epoch 00025: val\_loss did not improve
334/334 [==============================] - 405s - loss: 3.4440 - acc: 0.2156 - val\_loss: 3.8698 - val\_acc: 0.1377
Epoch 27/100
333/334 [============================>.] - ETA: 1s - loss: 3.4186 - acc: 0.2192Epoch 00026: val\_loss did not improve
334/334 [==============================] - 417s - loss: 3.4189 - acc: 0.2189 - val\_loss: 3.7109 - val\_acc: 0.1605
Epoch 28/100
333/334 [============================>.] - ETA: 1s - loss: 3.4017 - acc: 0.2162Epoch 00027: val\_loss did not improve
334/334 [==============================] - 409s - loss: 3.4035 - acc: 0.2159 - val\_loss: 3.8658 - val\_acc: 0.1473
Epoch 29/100
333/334 [============================>.] - ETA: 1s - loss: 3.3824 - acc: 0.2240Epoch 00028: val\_loss did not improve
334/334 [==============================] - 403s - loss: 3.3823 - acc: 0.2240 - val\_loss: 3.8234 - val\_acc: 0.1569
Epoch 30/100
333/334 [============================>.] - ETA: 1s - loss: 3.3545 - acc: 0.2276Epoch 00029: val\_loss did not improve
334/334 [==============================] - 392s - loss: 3.3535 - acc: 0.2275 - val\_loss: 3.7301 - val\_acc: 0.1737
Epoch 31/100
333/334 [============================>.] - ETA: 1s - loss: 3.3527 - acc: 0.2267Epoch 00030: val\_loss did not improve
334/334 [==============================] - 412s - loss: 3.3511 - acc: 0.2271 - val\_loss: 3.7524 - val\_acc: 0.1749
Epoch 32/100
333/334 [============================>.] - ETA: 1s - loss: 3.3191 - acc: 0.2270Epoch 00031: val\_loss did not improve
334/334 [==============================] - 411s - loss: 3.3192 - acc: 0.2268 - val\_loss: 3.8069 - val\_acc: 0.1641
Epoch 33/100
333/334 [============================>.] - ETA: 1s - loss: 3.3112 - acc: 0.2362Epoch 00032: val\_loss did not improve
334/334 [==============================] - 408s - loss: 3.3110 - acc: 0.2364 - val\_loss: 3.7622 - val\_acc: 0.1677
Epoch 34/100
333/334 [============================>.] - ETA: 1s - loss: 3.3105 - acc: 0.2311Epoch 00033: val\_loss did not improve
334/334 [==============================] - 399s - loss: 3.3107 - acc: 0.2310 - val\_loss: 3.7363 - val\_acc: 0.1473
Epoch 35/100
333/334 [============================>.] - ETA: 1s - loss: 3.2900 - acc: 0.2351Epoch 00034: val\_loss did not improve
334/334 [==============================] - 401s - loss: 3.2885 - acc: 0.2355 - val\_loss: 3.6869 - val\_acc: 0.1713
Epoch 36/100
333/334 [============================>.] - ETA: 1s - loss: 3.2773 - acc: 0.2380Epoch 00035: val\_loss did not improve
334/334 [==============================] - 418s - loss: 3.2769 - acc: 0.2382 - val\_loss: 3.7602 - val\_acc: 0.1425
Epoch 37/100
333/334 [============================>.] - ETA: 1s - loss: 3.2628 - acc: 0.2464Epoch 00036: val\_loss did not improve
334/334 [==============================] - 408s - loss: 3.2616 - acc: 0.2461 - val\_loss: 3.8143 - val\_acc: 0.1713
Epoch 38/100
333/334 [============================>.] - ETA: 1s - loss: 3.2263 - acc: 0.2492Epoch 00037: val\_loss improved from 3.68378 to 3.65667, saving model to saved\_models/weights.best.from\_scratch.hdf5
334/334 [==============================] - 412s - loss: 3.2261 - acc: 0.2491 - val\_loss: 3.6567 - val\_acc: 0.1665
Epoch 39/100
333/334 [============================>.] - ETA: 1s - loss: 3.2289 - acc: 0.2483Epoch 00038: val\_loss did not improve
334/334 [==============================] - 412s - loss: 3.2295 - acc: 0.2484 - val\_loss: 3.6793 - val\_acc: 0.1581
Epoch 40/100
333/334 [============================>.] - ETA: 1s - loss: 3.2365 - acc: 0.2468Epoch 00039: val\_loss improved from 3.65667 to 3.61789, saving model to saved\_models/weights.best.from\_scratch.hdf5
334/334 [==============================] - 412s - loss: 3.2367 - acc: 0.2469 - val\_loss: 3.6179 - val\_acc: 0.1820
Epoch 41/100
333/334 [============================>.] - ETA: 1s - loss: 3.1941 - acc: 0.2556Epoch 00040: val\_loss improved from 3.61789 to 3.55805, saving model to saved\_models/weights.best.from\_scratch.hdf5
334/334 [==============================] - 412s - loss: 3.1949 - acc: 0.2551 - val\_loss: 3.5581 - val\_acc: 0.1701
Epoch 42/100
333/334 [============================>.] - ETA: 1s - loss: 3.1868 - acc: 0.2578Epoch 00041: val\_loss did not improve
334/334 [==============================] - 410s - loss: 3.1871 - acc: 0.2578 - val\_loss: 3.6117 - val\_acc: 0.1856
Epoch 43/100
333/334 [============================>.] - ETA: 1s - loss: 3.1657 - acc: 0.2605Epoch 00042: val\_loss did not improve
334/334 [==============================] - 406s - loss: 3.1657 - acc: 0.2603 - val\_loss: 3.6380 - val\_acc: 0.1820
Epoch 44/100
333/334 [============================>.] - ETA: 1s - loss: 3.1704 - acc: 0.2572Epoch 00043: val\_loss did not improve
334/334 [==============================] - 421s - loss: 3.1698 - acc: 0.2575 - val\_loss: 3.6359 - val\_acc: 0.1940
Epoch 45/100
333/334 [============================>.] - ETA: 1s - loss: 3.1615 - acc: 0.2590Epoch 00044: val\_loss did not improve
334/334 [==============================] - 411s - loss: 3.1618 - acc: 0.2588 - val\_loss: 3.6505 - val\_acc: 0.2000
Epoch 46/100
333/334 [============================>.] - ETA: 1s - loss: 3.1311 - acc: 0.2691Epoch 00045: val\_loss did not improve
334/334 [==============================] - 408s - loss: 3.1312 - acc: 0.2693 - val\_loss: 3.6794 - val\_acc: 0.1916
Epoch 47/100
333/334 [============================>.] - ETA: 1s - loss: 3.1291 - acc: 0.2700Epoch 00046: val\_loss did not improve
334/334 [==============================] - 409s - loss: 3.1302 - acc: 0.2701 - val\_loss: 3.7396 - val\_acc: 0.1749
Epoch 48/100
333/334 [============================>.] - ETA: 1s - loss: 3.1058 - acc: 0.2710Epoch 00047: val\_loss did not improve
334/334 [==============================] - 408s - loss: 3.1043 - acc: 0.2716 - val\_loss: 3.5902 - val\_acc: 0.1964
Epoch 49/100
333/334 [============================>.] - ETA: 1s - loss: 3.1031 - acc: 0.2680Epoch 00048: val\_loss did not improve
334/334 [==============================] - 416s - loss: 3.1017 - acc: 0.2684 - val\_loss: 3.7230 - val\_acc: 0.1868
Epoch 50/100
333/334 [============================>.] - ETA: 1s - loss: 3.0793 - acc: 0.2754Epoch 00049: val\_loss did not improve
334/334 [==============================] - 410s - loss: 3.0794 - acc: 0.2751 - val\_loss: 4.0819 - val\_acc: 0.1377
Epoch 51/100
333/334 [============================>.] - ETA: 1s - loss: 3.0776 - acc: 0.2809Epoch 00050: val\_loss did not improve
334/334 [==============================] - 409s - loss: 3.0783 - acc: 0.2808 - val\_loss: 3.6956 - val\_acc: 0.1928
Epoch 52/100
333/334 [============================>.] - ETA: 1s - loss: 3.0508 - acc: 0.2854Epoch 00051: val\_loss did not improve
334/334 [==============================] - 409s - loss: 3.0505 - acc: 0.2852 - val\_loss: 3.6975 - val\_acc: 0.1689
Epoch 53/100
333/334 [============================>.] - ETA: 1s - loss: 3.0728 - acc: 0.2782Epoch 00052: val\_loss improved from 3.55805 to 3.52229, saving model to saved\_models/weights.best.from\_scratch.hdf5
334/334 [==============================] - 416s - loss: 3.0736 - acc: 0.2780 - val\_loss: 3.5223 - val\_acc: 0.2000
Epoch 54/100
333/334 [============================>.] - ETA: 1s - loss: 3.0605 - acc: 0.2805Epoch 00053: val\_loss did not improve
334/334 [==============================] - 410s - loss: 3.0628 - acc: 0.2799 - val\_loss: 3.6628 - val\_acc: 0.1904
Epoch 55/100
333/334 [============================>.] - ETA: 1s - loss: 3.0302 - acc: 0.2800Epoch 00054: val\_loss did not improve
334/334 [==============================] - 411s - loss: 3.0313 - acc: 0.2799 - val\_loss: 3.6789 - val\_acc: 0.1988
Epoch 56/100
333/334 [============================>.] - ETA: 1s - loss: 3.0465 - acc: 0.2808Epoch 00055: val\_loss did not improve
334/334 [==============================] - 405s - loss: 3.0461 - acc: 0.2808 - val\_loss: 3.6171 - val\_acc: 0.1940
Epoch 57/100
333/334 [============================>.] - ETA: 1s - loss: 3.0336 - acc: 0.2806Epoch 00056: val\_loss did not improve
334/334 [==============================] - 416s - loss: 3.0323 - acc: 0.2805 - val\_loss: 3.7747 - val\_acc: 0.1784
Epoch 58/100
333/334 [============================>.] - ETA: 1s - loss: 3.0076 - acc: 0.2866Epoch 00057: val\_loss did not improve
334/334 [==============================] - 414s - loss: 3.0078 - acc: 0.2862 - val\_loss: 3.7802 - val\_acc: 0.1856
Epoch 59/100
333/334 [============================>.] - ETA: 1s - loss: 3.0035 - acc: 0.2923Epoch 00058: val\_loss did not improve
334/334 [==============================] - 407s - loss: 3.0038 - acc: 0.2924 - val\_loss: 3.6676 - val\_acc: 0.1892
Epoch 60/100
333/334 [============================>.] - ETA: 1s - loss: 2.9900 - acc: 0.2941Epoch 00059: val\_loss did not improve
334/334 [==============================] - 409s - loss: 2.9891 - acc: 0.2945 - val\_loss: 3.7849 - val\_acc: 0.1868
Epoch 61/100
333/334 [============================>.] - ETA: 1s - loss: 2.9896 - acc: 0.2968Epoch 00060: val\_loss did not improve
334/334 [==============================] - 410s - loss: 2.9895 - acc: 0.2970 - val\_loss: 3.5493 - val\_acc: 0.2120
Epoch 62/100
333/334 [============================>.] - ETA: 1s - loss: 2.9803 - acc: 0.2983Epoch 00061: val\_loss did not improve
334/334 [==============================] - 434s - loss: 2.9807 - acc: 0.2981 - val\_loss: 3.8103 - val\_acc: 0.1856
Epoch 63/100
333/334 [============================>.] - ETA: 1s - loss: 2.9724 - acc: 0.2961Epoch 00062: val\_loss did not improve
334/334 [==============================] - 422s - loss: 2.9734 - acc: 0.2960 - val\_loss: 3.7554 - val\_acc: 0.1856
Epoch 64/100
333/334 [============================>.] - ETA: 1s - loss: 2.9775 - acc: 0.2983Epoch 00063: val\_loss did not improve
334/334 [==============================] - 420s - loss: 2.9763 - acc: 0.2982 - val\_loss: 3.8452 - val\_acc: 0.1617
Epoch 65/100
333/334 [============================>.] - ETA: 1s - loss: 2.9573 - acc: 0.3015Epoch 00064: val\_loss did not improve
334/334 [==============================] - 428s - loss: 2.9576 - acc: 0.3013 - val\_loss: 3.6732 - val\_acc: 0.1796
Epoch 66/100
333/334 [============================>.] - ETA: 1s - loss: 2.9586 - acc: 0.2973Epoch 00065: val\_loss did not improve
334/334 [==============================] - 431s - loss: 2.9586 - acc: 0.2967 - val\_loss: 4.0999 - val\_acc: 0.1629
Epoch 67/100
333/334 [============================>.] - ETA: 1s - loss: 2.9376 - acc: 0.3015Epoch 00066: val\_loss did not improve
334/334 [==============================] - 407s - loss: 2.9379 - acc: 0.3015 - val\_loss: 3.6695 - val\_acc: 0.1928
Epoch 68/100
333/334 [============================>.] - ETA: 1s - loss: 2.9501 - acc: 0.2983Epoch 00067: val\_loss improved from 3.52229 to 3.50435, saving model to saved\_models/weights.best.from\_scratch.hdf5
334/334 [==============================] - 405s - loss: 2.9503 - acc: 0.2984 - val\_loss: 3.5044 - val\_acc: 0.1940
Epoch 69/100
333/334 [============================>.] - ETA: 1s - loss: 2.9425 - acc: 0.2973Epoch 00068: val\_loss did not improve
334/334 [==============================] - 404s - loss: 2.9422 - acc: 0.2978 - val\_loss: 3.9888 - val\_acc: 0.1725
Epoch 70/100
333/334 [============================>.] - ETA: 1s - loss: 2.9264 - acc: 0.3054Epoch 00069: val\_loss did not improve
334/334 [==============================] - 415s - loss: 2.9279 - acc: 0.3051 - val\_loss: 3.8521 - val\_acc: 0.1713
Epoch 71/100
333/334 [============================>.] - ETA: 1s - loss: 2.9285 - acc: 0.3087Epoch 00070: val\_loss did not improve
334/334 [==============================] - 411s - loss: 2.9283 - acc: 0.3090 - val\_loss: 3.5560 - val\_acc: 0.1904
Epoch 72/100
333/334 [============================>.] - ETA: 1s - loss: 2.9017 - acc: 0.3050Epoch 00071: val\_loss did not improve
334/334 [==============================] - 406s - loss: 2.9018 - acc: 0.3051 - val\_loss: 3.8380 - val\_acc: 0.1856
Epoch 73/100
333/334 [============================>.] - ETA: 1s - loss: 2.9064 - acc: 0.3042Epoch 00072: val\_loss did not improve
334/334 [==============================] - 409s - loss: 2.9064 - acc: 0.3043 - val\_loss: 3.5495 - val\_acc: 0.1952
Epoch 74/100
333/334 [============================>.] - ETA: 1s - loss: 2.8962 - acc: 0.3081Epoch 00073: val\_loss did not improve
334/334 [==============================] - 408s - loss: 2.8978 - acc: 0.3081 - val\_loss: 3.6736 - val\_acc: 0.1988
Epoch 75/100
333/334 [============================>.] - ETA: 1s - loss: 2.9172 - acc: 0.3086Epoch 00074: val\_loss did not improve
334/334 [==============================] - 411s - loss: 2.9181 - acc: 0.3085 - val\_loss: 3.7647 - val\_acc: 0.1940
Epoch 76/100
333/334 [============================>.] - ETA: 1s - loss: 2.8978 - acc: 0.3084Epoch 00075: val\_loss did not improve
334/334 [==============================] - 409s - loss: 2.8963 - acc: 0.3084 - val\_loss: 3.5874 - val\_acc: 0.1988
Epoch 77/100
333/334 [============================>.] - ETA: 1s - loss: 2.8857 - acc: 0.3122Epoch 00076: val\_loss improved from 3.50435 to 3.49061, saving model to saved\_models/weights.best.from\_scratch.hdf5
334/334 [==============================] - 409s - loss: 2.8874 - acc: 0.3118 - val\_loss: 3.4906 - val\_acc: 0.2192
Epoch 78/100
333/334 [============================>.] - ETA: 1s - loss: 2.8630 - acc: 0.3167Epoch 00077: val\_loss did not improve
334/334 [==============================] - 405s - loss: 2.8630 - acc: 0.3163 - val\_loss: 3.8922 - val\_acc: 0.1713
Epoch 79/100
333/334 [============================>.] - ETA: 1s - loss: 2.8798 - acc: 0.3132Epoch 00078: val\_loss did not improve
334/334 [==============================] - 429s - loss: 2.8810 - acc: 0.3124 - val\_loss: 3.6242 - val\_acc: 0.1952
Epoch 80/100
333/334 [============================>.] - ETA: 1s - loss: 2.8580 - acc: 0.3176Epoch 00079: val\_loss did not improve
334/334 [==============================] - 425s - loss: 2.8600 - acc: 0.3172 - val\_loss: 3.7260 - val\_acc: 0.2012
Epoch 81/100
333/334 [============================>.] - ETA: 1s - loss: 2.8755 - acc: 0.3156Epoch 00080: val\_loss did not improve
334/334 [==============================] - 421s - loss: 2.8753 - acc: 0.3153 - val\_loss: 3.6563 - val\_acc: 0.1832
Epoch 82/100
333/334 [============================>.] - ETA: 1s - loss: 2.8457 - acc: 0.3179Epoch 00081: val\_loss did not improve
334/334 [==============================] - 415s - loss: 2.8473 - acc: 0.3178 - val\_loss: 3.6109 - val\_acc: 0.2096
Epoch 83/100
333/334 [============================>.] - ETA: 1s - loss: 2.8502 - acc: 0.3165Epoch 00082: val\_loss did not improve
334/334 [==============================] - 420s - loss: 2.8519 - acc: 0.3159 - val\_loss: 3.6484 - val\_acc: 0.1856
Epoch 84/100
333/334 [============================>.] - ETA: 1s - loss: 2.8464 - acc: 0.3288Epoch 00083: val\_loss did not improve
334/334 [==============================] - 411s - loss: 2.8465 - acc: 0.3292 - val\_loss: 3.7338 - val\_acc: 0.1916
Epoch 85/100
333/334 [============================>.] - ETA: 1s - loss: 2.8467 - acc: 0.3194Epoch 00084: val\_loss did not improve
334/334 [==============================] - 406s - loss: 2.8485 - acc: 0.3189 - val\_loss: 3.5129 - val\_acc: 0.2108
Epoch 86/100
333/334 [============================>.] - ETA: 1s - loss: 2.8218 - acc: 0.3257Epoch 00085: val\_loss did not improve
334/334 [==============================] - 404s - loss: 2.8229 - acc: 0.3251 - val\_loss: 3.7789 - val\_acc: 0.1928
Epoch 87/100
333/334 [============================>.] - ETA: 1s - loss: 2.8527 - acc: 0.3171Epoch 00086: val\_loss did not improve
334/334 [==============================] - 402s - loss: 2.8525 - acc: 0.3169 - val\_loss: 3.5402 - val\_acc: 0.2204
Epoch 88/100
333/334 [============================>.] - ETA: 1s - loss: 2.8434 - acc: 0.3252Epoch 00087: val\_loss did not improve
334/334 [==============================] - 413s - loss: 2.8430 - acc: 0.3254 - val\_loss: 3.5896 - val\_acc: 0.2072
Epoch 89/100
333/334 [============================>.] - ETA: 1s - loss: 2.8270 - acc: 0.3246Epoch 00088: val\_loss did not improve
334/334 [==============================] - 412s - loss: 2.8277 - acc: 0.3243 - val\_loss: 3.5299 - val\_acc: 0.2096
Epoch 90/100
333/334 [============================>.] - ETA: 1s - loss: 2.8219 - acc: 0.3260Epoch 00089: val\_loss did not improve
334/334 [==============================] - 411s - loss: 2.8237 - acc: 0.3259 - val\_loss: 3.6818 - val\_acc: 0.1832
Epoch 91/100
333/334 [============================>.] - ETA: 1s - loss: 2.7970 - acc: 0.3345Epoch 00090: val\_loss improved from 3.49061 to 3.48149, saving model to saved\_models/weights.best.from\_scratch.hdf5
334/334 [==============================] - 407s - loss: 2.7965 - acc: 0.3346 - val\_loss: 3.4815 - val\_acc: 0.2132
Epoch 92/100
333/334 [============================>.] - ETA: 1s - loss: 2.7765 - acc: 0.3392Epoch 00091: val\_loss improved from 3.48149 to 3.43112, saving model to saved\_models/weights.best.from\_scratch.hdf5
334/334 [==============================] - 418s - loss: 2.7775 - acc: 0.3391 - val\_loss: 3.4311 - val\_acc: 0.2251
Epoch 93/100
333/334 [============================>.] - ETA: 1s - loss: 2.7948 - acc: 0.3321Epoch 00092: val\_loss did not improve
334/334 [==============================] - 410s - loss: 2.7947 - acc: 0.3320 - val\_loss: 3.4946 - val\_acc: 0.2204
Epoch 94/100
333/334 [============================>.] - ETA: 1s - loss: 2.7947 - acc: 0.3308Epoch 00093: val\_loss did not improve
334/334 [==============================] - 406s - loss: 2.7954 - acc: 0.3307 - val\_loss: 3.4774 - val\_acc: 0.2263
Epoch 95/100
333/334 [============================>.] - ETA: 1s - loss: 2.7850 - acc: 0.3308Epoch 00094: val\_loss did not improve
334/334 [==============================] - 413s - loss: 2.7854 - acc: 0.3305 - val\_loss: 3.7213 - val\_acc: 0.1904
Epoch 96/100
333/334 [============================>.] - ETA: 1s - loss: 2.7992 - acc: 0.3233Epoch 00095: val\_loss did not improve
334/334 [==============================] - 411s - loss: 2.7996 - acc: 0.3232 - val\_loss: 3.6502 - val\_acc: 0.1760
Epoch 97/100
333/334 [============================>.] - ETA: 1s - loss: 2.7699 - acc: 0.3366Epoch 00096: val\_loss did not improve
334/334 [==============================] - 421s - loss: 2.7698 - acc: 0.3365 - val\_loss: 3.6428 - val\_acc: 0.2060
Epoch 98/100
333/334 [============================>.] - ETA: 1s - loss: 2.7581 - acc: 0.3414Epoch 00097: val\_loss did not improve
334/334 [==============================] - 408s - loss: 2.7565 - acc: 0.3421 - val\_loss: 3.5285 - val\_acc: 0.2000
Epoch 99/100
333/334 [============================>.] - ETA: 1s - loss: 2.7804 - acc: 0.3288Epoch 00098: val\_loss did not improve
334/334 [==============================] - 411s - loss: 2.7828 - acc: 0.3281 - val\_loss: 3.5835 - val\_acc: 0.2000
Epoch 100/100
333/334 [============================>.] - ETA: 1s - loss: 2.7544 - acc: 0.3434Epoch 00099: val\_loss did not improve
334/334 [==============================] - 408s - loss: 2.7540 - acc: 0.3433 - val\_loss: 3.5844 - val\_acc: 0.2156

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}9}]:} <keras.callbacks.History at 0x7fea2bc08f60>
\end{Verbatim}
            
    \hypertarget{load-the-model-with-the-best-validation-loss}{%
\subsubsection{Load the Model with the Best Validation
Loss}\label{load-the-model-with-the-best-validation-loss}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{n}{model}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{saved\PYZus{}models/weights.best.from\PYZus{}scratch.hdf5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \hypertarget{test-the-model}{%
\subsubsection{Test the Model}\label{test-the-model}}

Try out your model on the test dataset of dog images. Ensure that your
test accuracy is greater than 1\%.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{c+c1}{\PYZsh{} get index of predicted dog breed for each image in test set}
         \PY{n}{dog\PYZus{}breed\PYZus{}predictions} \PY{o}{=} \PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{expand\PYZus{}dims}\PY{p}{(}\PY{n}{tensor}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{)}\PY{p}{)} \PY{k}{for} \PY{n}{tensor} \PY{o+ow}{in} \PY{n}{test\PYZus{}tensors}\PY{p}{]}
         
         \PY{c+c1}{\PYZsh{} report test accuracy}
         \PY{n}{test\PYZus{}accuracy} \PY{o}{=} \PY{l+m+mi}{100}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{dog\PYZus{}breed\PYZus{}predictions}\PY{p}{)}\PY{o}{==}\PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{test\PYZus{}targets}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{o}{/}\PY{n+nb}{len}\PY{p}{(}\PY{n}{dog\PYZus{}breed\PYZus{}predictions}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test accuracy: }\PY{l+s+si}{\PYZpc{}.4f}\PY{l+s+si}{\PYZpc{}\PYZpc{}}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n}{test\PYZus{}accuracy}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Test accuracy: 23.0861\%

    \end{Verbatim}

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

 \#\# Step 4: Use a CNN to Classify Dog Breeds

To reduce training time without sacrificing accuracy, we show you how to
train a CNN using transfer learning. In the following step, you will get
a chance to use transfer learning to train your own CNN.

\hypertarget{obtain-bottleneck-features}{%
\subsubsection{Obtain Bottleneck
Features}\label{obtain-bottleneck-features}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{n}{bottleneck\PYZus{}features} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{load}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bottleneck\PYZus{}features/DogVGG16Data.npz}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{train\PYZus{}VGG16} \PY{o}{=} \PY{n}{bottleneck\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
         \PY{n}{valid\PYZus{}VGG16} \PY{o}{=} \PY{n}{bottleneck\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{valid}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
         \PY{n}{test\PYZus{}VGG16} \PY{o}{=} \PY{n}{bottleneck\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
\end{Verbatim}


    \hypertarget{model-architecture}{%
\subsubsection{Model Architecture}\label{model-architecture}}

The model uses the the pre-trained VGG-16 model as a fixed feature
extractor, where the last convolutional output of VGG-16 is fed as input
to our model. We only add a global average pooling layer and a fully
connected layer, where the latter contains one node for each dog
category and is equipped with a softmax.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{n}{VGG16\PYZus{}model} \PY{o}{=} \PY{n}{Sequential}\PY{p}{(}\PY{p}{)}
         \PY{n}{VGG16\PYZus{}model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{GlobalAveragePooling2D}\PY{p}{(}\PY{n}{input\PYZus{}shape}\PY{o}{=}\PY{n}{train\PYZus{}VGG16}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{)}
         \PY{n}{VGG16\PYZus{}model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{133}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{softmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{VGG16\PYZus{}model}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                 Output Shape              Param \#   
=================================================================
global\_average\_pooling2d\_2 ( (None, 512)               0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_2 (Dense)              (None, 133)               68229     
=================================================================
Total params: 68,229.0
Trainable params: 68,229.0
Non-trainable params: 0.0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

    \end{Verbatim}

    \hypertarget{compile-the-model}{%
\subsubsection{Compile the Model}\label{compile-the-model}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{n}{VGG16\PYZus{}model}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{categorical\PYZus{}crossentropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{optimizer}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{rmsprop}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \hypertarget{train-the-model}{%
\subsubsection{Train the Model}\label{train-the-model}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}16}]:} \PY{n}{checkpointer} \PY{o}{=} \PY{n}{ModelCheckpoint}\PY{p}{(}\PY{n}{filepath}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{saved\PYZus{}models/weights.best.VGG16.hdf5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                                        \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{save\PYZus{}best\PYZus{}only}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
         
         \PY{n}{VGG16\PYZus{}model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{train\PYZus{}VGG16}\PY{p}{,} \PY{n}{train\PYZus{}targets}\PY{p}{,} 
                   \PY{n}{validation\PYZus{}data}\PY{o}{=}\PY{p}{(}\PY{n}{valid\PYZus{}VGG16}\PY{p}{,} \PY{n}{valid\PYZus{}targets}\PY{p}{)}\PY{p}{,}
                   \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{checkpointer}\PY{p}{]}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Train on 6680 samples, validate on 835 samples
Epoch 1/20
6520/6680 [============================>.] - ETA: 0s - loss: 12.2800 - acc: 0.1144Epoch 00000: val\_loss improved from inf to 10.70108, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 1s - loss: 12.2218 - acc: 0.1171 - val\_loss: 10.7011 - val\_acc: 0.2299
Epoch 2/20
6540/6680 [============================>.] - ETA: 0s - loss: 10.4144 - acc: 0.2687Epoch 00001: val\_loss improved from 10.70108 to 10.45208, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 1s - loss: 10.3916 - acc: 0.2699 - val\_loss: 10.4521 - val\_acc: 0.2790
Epoch 3/20
6560/6680 [============================>.] - ETA: 0s - loss: 10.0273 - acc: 0.3253Epoch 00002: val\_loss improved from 10.45208 to 10.38357, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 2s - loss: 10.0309 - acc: 0.3247 - val\_loss: 10.3836 - val\_acc: 0.2826
Epoch 4/20
6560/6680 [============================>.] - ETA: 0s - loss: 9.8903 - acc: 0.3486Epoch 00003: val\_loss improved from 10.38357 to 10.19819, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 1s - loss: 9.9013 - acc: 0.3475 - val\_loss: 10.1982 - val\_acc: 0.3114
Epoch 5/20
6480/6680 [============================>.] - ETA: 0s - loss: 9.7890 - acc: 0.3628Epoch 00004: val\_loss improved from 10.19819 to 10.02958, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 1s - loss: 9.7750 - acc: 0.3633 - val\_loss: 10.0296 - val\_acc: 0.3186
Epoch 6/20
6460/6680 [============================>.] - ETA: 0s - loss: 9.6599 - acc: 0.3759Epoch 00005: val\_loss improved from 10.02958 to 10.02806, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 1s - loss: 9.6419 - acc: 0.3766 - val\_loss: 10.0281 - val\_acc: 0.3102
Epoch 7/20
6520/6680 [============================>.] - ETA: 0s - loss: 9.4375 - acc: 0.3867Epoch 00006: val\_loss improved from 10.02806 to 9.71643, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 1s - loss: 9.4444 - acc: 0.3867 - val\_loss: 9.7164 - val\_acc: 0.3317
Epoch 8/20
6600/6680 [============================>.] - ETA: 0s - loss: 9.1093 - acc: 0.4045Epoch 00007: val\_loss improved from 9.71643 to 9.53892, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 1s - loss: 9.1098 - acc: 0.4043 - val\_loss: 9.5389 - val\_acc: 0.3365
Epoch 9/20
6560/6680 [============================>.] - ETA: 0s - loss: 8.9423 - acc: 0.4259Epoch 00008: val\_loss improved from 9.53892 to 9.40647, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 1s - loss: 8.9405 - acc: 0.4259 - val\_loss: 9.4065 - val\_acc: 0.3461
Epoch 10/20
6620/6680 [============================>.] - ETA: 0s - loss: 8.7747 - acc: 0.4331Epoch 00009: val\_loss improved from 9.40647 to 9.24217, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 2s - loss: 8.7763 - acc: 0.4329 - val\_loss: 9.2422 - val\_acc: 0.3629
Epoch 11/20
6600/6680 [============================>.] - ETA: 0s - loss: 8.5849 - acc: 0.4492Epoch 00010: val\_loss improved from 9.24217 to 9.00448, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 1s - loss: 8.5845 - acc: 0.4491 - val\_loss: 9.0045 - val\_acc: 0.3760
Epoch 12/20
6620/6680 [============================>.] - ETA: 0s - loss: 8.4582 - acc: 0.4622Epoch 00011: val\_loss improved from 9.00448 to 8.99148, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 1s - loss: 8.4630 - acc: 0.4615 - val\_loss: 8.9915 - val\_acc: 0.3665
Epoch 13/20
6580/6680 [============================>.] - ETA: 0s - loss: 8.4043 - acc: 0.4684Epoch 00012: val\_loss improved from 8.99148 to 8.96471, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 1s - loss: 8.4152 - acc: 0.4677 - val\_loss: 8.9647 - val\_acc: 0.3784
Epoch 14/20
6660/6680 [============================>.] - ETA: 0s - loss: 8.2620 - acc: 0.4740Epoch 00013: val\_loss improved from 8.96471 to 8.88044, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 1s - loss: 8.2624 - acc: 0.4738 - val\_loss: 8.8804 - val\_acc: 0.3880
Epoch 15/20
6520/6680 [============================>.] - ETA: 0s - loss: 8.1280 - acc: 0.4816Epoch 00014: val\_loss improved from 8.88044 to 8.74951, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 1s - loss: 8.1253 - acc: 0.4819 - val\_loss: 8.7495 - val\_acc: 0.3904
Epoch 16/20
6560/6680 [============================>.] - ETA: 0s - loss: 8.0808 - acc: 0.4912Epoch 00015: val\_loss improved from 8.74951 to 8.60813, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 2s - loss: 8.0830 - acc: 0.4910 - val\_loss: 8.6081 - val\_acc: 0.4060
Epoch 17/20
6600/6680 [============================>.] - ETA: 0s - loss: 8.0069 - acc: 0.4923Epoch 00016: val\_loss improved from 8.60813 to 8.60763, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 2s - loss: 8.0050 - acc: 0.4919 - val\_loss: 8.6076 - val\_acc: 0.4000
Epoch 18/20
6580/6680 [============================>.] - ETA: 0s - loss: 7.9122 - acc: 0.4985Epoch 00017: val\_loss improved from 8.60763 to 8.51763, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 1s - loss: 7.9044 - acc: 0.4990 - val\_loss: 8.5176 - val\_acc: 0.4204
Epoch 19/20
6640/6680 [============================>.] - ETA: 0s - loss: 7.8613 - acc: 0.5042Epoch 00018: val\_loss did not improve
6680/6680 [==============================] - 1s - loss: 7.8678 - acc: 0.5037 - val\_loss: 8.5293 - val\_acc: 0.4108
Epoch 20/20
6560/6680 [============================>.] - ETA: 0s - loss: 7.7995 - acc: 0.5078Epoch 00019: val\_loss improved from 8.51763 to 8.38967, saving model to saved\_models/weights.best.VGG16.hdf5
6680/6680 [==============================] - 1s - loss: 7.8218 - acc: 0.5063 - val\_loss: 8.3897 - val\_acc: 0.4096

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}16}]:} <keras.callbacks.History at 0x7fea1072fbe0>
\end{Verbatim}
            
    \hypertarget{load-the-model-with-the-best-validation-loss}{%
\subsubsection{Load the Model with the Best Validation
Loss}\label{load-the-model-with-the-best-validation-loss}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}17}]:} \PY{n}{VGG16\PYZus{}model}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{saved\PYZus{}models/weights.best.VGG16.hdf5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \hypertarget{test-the-model}{%
\subsubsection{Test the Model}\label{test-the-model}}

Now, we can use the CNN to test how well it identifies breed within our
test dataset of dog images. We print the test accuracy below.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}18}]:} \PY{c+c1}{\PYZsh{} get index of predicted dog breed for each image in test set}
         \PY{n}{VGG16\PYZus{}predictions} \PY{o}{=} \PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{VGG16\PYZus{}model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{expand\PYZus{}dims}\PY{p}{(}\PY{n}{feature}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{)}\PY{p}{)} \PY{k}{for} \PY{n}{feature} \PY{o+ow}{in} \PY{n}{test\PYZus{}VGG16}\PY{p}{]}
         
         \PY{c+c1}{\PYZsh{} report test accuracy}
         \PY{n}{test\PYZus{}accuracy} \PY{o}{=} \PY{l+m+mi}{100}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{VGG16\PYZus{}predictions}\PY{p}{)}\PY{o}{==}\PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{test\PYZus{}targets}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{o}{/}\PY{n+nb}{len}\PY{p}{(}\PY{n}{VGG16\PYZus{}predictions}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test accuracy: }\PY{l+s+si}{\PYZpc{}.4f}\PY{l+s+si}{\PYZpc{}\PYZpc{}}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n}{test\PYZus{}accuracy}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Test accuracy: 39.7129\%

    \end{Verbatim}

    \hypertarget{predict-dog-breed-with-the-model}{%
\subsubsection{Predict Dog Breed with the
Model}\label{predict-dog-breed-with-the-model}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}19}]:} \PY{k+kn}{from} \PY{n+nn}{extract\PYZus{}bottleneck\PYZus{}features} \PY{k}{import} \PY{o}{*}
         
         \PY{k}{def} \PY{n+nf}{VGG16\PYZus{}predict\PYZus{}breed}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}\PY{p}{:}
             \PY{c+c1}{\PYZsh{} extract bottleneck features}
             \PY{n}{bottleneck\PYZus{}feature} \PY{o}{=} \PY{n}{extract\PYZus{}VGG16}\PY{p}{(}\PY{n}{path\PYZus{}to\PYZus{}tensor}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} obtain predicted vector}
             \PY{n}{predicted\PYZus{}vector} \PY{o}{=} \PY{n}{VGG16\PYZus{}model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{bottleneck\PYZus{}feature}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} return dog breed that is predicted by the model}
             \PY{k}{return} \PY{n}{dog\PYZus{}names}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{predicted\PYZus{}vector}\PY{p}{)}\PY{p}{]}
\end{Verbatim}


    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

 \#\# Step 5: Create a CNN to Classify Dog Breeds (using Transfer
Learning)

You will now use transfer learning to create a CNN that can identify dog
breed from images. Your CNN must attain at least 60\% accuracy on the
test set.

In Step 4, we used transfer learning to create a CNN using VGG-16
bottleneck features. In this section, you must use the bottleneck
features from a different pre-trained model. To make things easier for
you, we have pre-computed the features for all of the networks that are
currently available in Keras: -
\href{https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogVGG19Data.npz}{VGG-19}
bottleneck features -
\href{https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogResnet50Data.npz}{ResNet-50}
bottleneck features -
\href{https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogInceptionV3Data.npz}{Inception}
bottleneck features -
\href{https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogXceptionData.npz}{Xception}
bottleneck features

The files are encoded as such:

\begin{verbatim}
Dog{network}Data.npz
\end{verbatim}

where \texttt{\{network\}}, in the above filename, can be one of
\texttt{VGG19}, \texttt{Resnet50}, \texttt{InceptionV3}, or
\texttt{Xception}. Pick one of the above architectures, download the
corresponding bottleneck features, and store the downloaded file in the
\texttt{bottleneck\_features/} folder in the repository.

\hypertarget{implementation-obtain-bottleneck-features}{%
\subsubsection{(IMPLEMENTATION) Obtain Bottleneck
Features}\label{implementation-obtain-bottleneck-features}}

In the code block below, extract the bottleneck features corresponding
to the train, test, and validation sets by running the following:

\begin{verbatim}
bottleneck_features = np.load('bottleneck_features/Dog{network}Data.npz')
train_{network} = bottleneck_features['train']
valid_{network} = bottleneck_features['valid']
test_{network} = bottleneck_features['test']
\end{verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}22}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} TODO: Obtain bottleneck features from another pre\PYZhy{}trained CNN.}
         \PY{k}{def} \PY{n+nf}{load\PYZus{}bottleneck\PYZus{}features}\PY{p}{(}\PY{n}{network}\PY{p}{)}\PY{p}{:}
             \PY{n}{bottleneck\PYZus{}features} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{load}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bottleneck\PYZus{}features/Dog}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{Data.npz}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{network}\PY{p}{)}\PY{p}{)}
             \PY{k}{return} \PY{n}{bottleneck\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}\PY{n}{bottleneck\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{valid}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}\PY{n}{bottleneck\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
         
         \PY{n}{train\PYZus{}vgg19}\PY{p}{,}\PY{n}{valid\PYZus{}vgg19}\PY{p}{,}\PY{n}{test\PYZus{}vgg19} \PY{o}{=} \PY{n}{load\PYZus{}bottleneck\PYZus{}features}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{VGG19}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{train\PYZus{}resnet50}\PY{p}{,}\PY{n}{valid\PYZus{}resnet50}\PY{p}{,}\PY{n}{test\PYZus{}resnet50} \PY{o}{=} \PY{n}{load\PYZus{}bottleneck\PYZus{}features}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Resnet50}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{train\PYZus{}xception}\PY{p}{,}\PY{n}{valid\PYZus{}xception}\PY{p}{,}\PY{n}{test\PYZus{}xception} \PY{o}{=} \PY{n}{load\PYZus{}bottleneck\PYZus{}features}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Xception}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{train\PYZus{}inceptionv3}\PY{p}{,}\PY{n}{valid\PYZus{}inceptionv3}\PY{p}{,}\PY{n}{test\PYZus{}inceptionv3} \PY{o}{=} \PY{n}{load\PYZus{}bottleneck\PYZus{}features}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{InceptionV3}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \hypertarget{implementation-model-architecture}{%
\subsubsection{(IMPLEMENTATION) Model
Architecture}\label{implementation-model-architecture}}

Create a CNN to classify dog breed. At the end of your code cell block,
summarize the layers of your model by executing the line:

\begin{verbatim}
    <your model's name>.summary()
\end{verbatim}

\textbf{Question 5:} Outline the steps you took to get to your final CNN
architecture and your reasoning at each step. Describe why you think the
architecture is suitable for the current problem.

\textbf{Answer:}

Steps: 1. Defined GAP-\textgreater{}Dense. Xception gave me 85\%. That's
fine, I'll use this model.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}26}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} TODO: Define your architecture.}
         
         \PY{n}{vgg19\PYZus{}model}\PY{p}{,} \PY{n}{resnet50\PYZus{}model}\PY{p}{,} \PY{n}{xception\PYZus{}model}\PY{p}{,} \PY{n}{inceptionv3\PYZus{}model} \PY{o}{=} \PY{k+kc}{None}\PY{p}{,}\PY{k+kc}{None}\PY{p}{,}\PY{k+kc}{None}\PY{p}{,}\PY{k+kc}{None}
         
         \PY{k}{def} \PY{n+nf}{define\PYZus{}model}\PY{p}{(}\PY{n}{modeo}\PY{p}{,}\PY{n}{inpt}\PY{p}{)}\PY{p}{:}
             \PY{n}{modeo} \PY{o}{=} \PY{n}{Sequential}\PY{p}{(}\PY{p}{)}
             \PY{n}{modeo}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{GlobalAveragePooling2D}\PY{p}{(}\PY{n}{input\PYZus{}shape}\PY{o}{=}\PY{n}{inpt}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{)}
             \PY{n}{modeo}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{133}\PY{p}{,}\PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{softmax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
             \PY{n}{modeo}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}
             \PY{k}{return} \PY{n}{modeo}
         
         \PY{n}{vgg19\PYZus{}model} \PY{o}{=} \PY{n}{define\PYZus{}model}\PY{p}{(}\PY{n}{vgg19\PYZus{}model}\PY{p}{,}\PY{n}{train\PYZus{}vgg19}\PY{p}{)}
         \PY{n}{resnet50\PYZus{}model} \PY{o}{=} \PY{n}{define\PYZus{}model}\PY{p}{(}\PY{n}{resnet50\PYZus{}model}\PY{p}{,}\PY{n}{train\PYZus{}resnet50}\PY{p}{)}
         \PY{n}{xception\PYZus{}model} \PY{o}{=} \PY{n}{define\PYZus{}model}\PY{p}{(}\PY{n}{xception\PYZus{}model}\PY{p}{,}\PY{n}{train\PYZus{}xception}\PY{p}{)}
         \PY{n}{inceptionv3\PYZus{}model} \PY{o}{=} \PY{n}{define\PYZus{}model}\PY{p}{(}\PY{n}{inceptionv3\PYZus{}model}\PY{p}{,}\PY{n}{train\PYZus{}inceptionv3}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                 Output Shape              Param \#   
=================================================================
global\_average\_pooling2d\_11  (None, 512)               0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_11 (Dense)             (None, 133)               68229     
=================================================================
Total params: 68,229.0
Trainable params: 68,229.0
Non-trainable params: 0.0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                 Output Shape              Param \#   
=================================================================
global\_average\_pooling2d\_12  (None, 2048)              0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_12 (Dense)             (None, 133)               272517    
=================================================================
Total params: 272,517.0
Trainable params: 272,517.0
Non-trainable params: 0.0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                 Output Shape              Param \#   
=================================================================
global\_average\_pooling2d\_13  (None, 2048)              0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_13 (Dense)             (None, 133)               272517    
=================================================================
Total params: 272,517.0
Trainable params: 272,517.0
Non-trainable params: 0.0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                 Output Shape              Param \#   
=================================================================
global\_average\_pooling2d\_14  (None, 2048)              0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_14 (Dense)             (None, 133)               272517    
=================================================================
Total params: 272,517.0
Trainable params: 272,517.0
Non-trainable params: 0.0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

    \end{Verbatim}

    \hypertarget{implementation-compile-the-model}{%
\subsubsection{(IMPLEMENTATION) Compile the
Model}\label{implementation-compile-the-model}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}27}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} TODO: Compile the model.}
         \PY{k}{def} \PY{n+nf}{compile\PYZus{}model}\PY{p}{(}\PY{n}{modeo}\PY{p}{)}\PY{p}{:}
             \PY{n}{modeo}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{categorical\PYZus{}crossentropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{optimizer}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{rmsprop}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         
         \PY{n}{compile\PYZus{}model}\PY{p}{(}\PY{n}{vgg19\PYZus{}model}\PY{p}{)}
         \PY{n}{compile\PYZus{}model}\PY{p}{(}\PY{n}{resnet50\PYZus{}model}\PY{p}{)}
         \PY{n}{compile\PYZus{}model}\PY{p}{(}\PY{n}{xception\PYZus{}model}\PY{p}{)}
         \PY{n}{compile\PYZus{}model}\PY{p}{(}\PY{n}{inceptionv3\PYZus{}model}\PY{p}{)}
\end{Verbatim}


    \hypertarget{implementation-train-the-model}{%
\subsubsection{(IMPLEMENTATION) Train the
Model}\label{implementation-train-the-model}}

Train your model in the code cell below. Use model checkpointing to save
the model that attains the best validation loss.

You are welcome to
\href{https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html}{augment
the training data}, but this is not a requirement.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}40}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} TODO: Train the model.}
         \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{20}
         \PY{n}{epochs} \PY{o}{=} \PY{l+m+mi}{20}
         \PY{k}{def} \PY{n+nf}{train\PYZus{}model}\PY{p}{(}\PY{n}{modeo}\PY{p}{,}\PY{n}{checkpoint\PYZus{}file}\PY{p}{,}\PY{n}{trn\PYZus{}tensors}\PY{p}{,}\PY{n}{trn\PYZus{}targets}\PY{p}{,}\PY{n}{vld\PYZus{}tensors}\PY{p}{,}\PY{n}{vld\PYZus{}targets}\PY{p}{)}\PY{p}{:}
             \PY{k}{global} \PY{n}{batch\PYZus{}size} 
             \PY{k}{global} \PY{n}{epochs}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{Training with }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{ algorithm}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{checkpoint\PYZus{}file}\PY{o}{.}\PY{n}{upper}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{)}
             \PY{n}{chk} \PY{o}{=} \PY{n}{ModelCheckpoint}\PY{p}{(}\PY{n}{filepath}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{saved\PYZus{}models/weights.best.}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{.hdf5}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{checkpoint\PYZus{}file}\PY{p}{)}\PY{p}{,}
                                  \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{save\PYZus{}best\PYZus{}only}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
             \PY{n}{modeo}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{trn\PYZus{}tensors}\PY{p}{,} \PY{n}{trn\PYZus{}targets}\PY{p}{,} 
                   \PY{n}{validation\PYZus{}data}\PY{o}{=}\PY{p}{(}\PY{n}{vld\PYZus{}tensors}\PY{p}{,} \PY{n}{vld\PYZus{}targets}\PY{p}{)}\PY{p}{,}
                   \PY{n}{epochs}\PY{o}{=}\PY{n}{epochs}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{n}{batch\PYZus{}size}\PY{p}{,} \PY{n}{callbacks}\PY{o}{=}\PY{p}{[}\PY{n}{chk}\PY{p}{]}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}
             
         \PY{n}{train\PYZus{}model}\PY{p}{(}\PY{n}{vgg19\PYZus{}model}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{vgg19}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{train\PYZus{}vgg19}\PY{p}{,}\PY{n}{train\PYZus{}targets}\PY{p}{,}\PY{n}{valid\PYZus{}vgg19}\PY{p}{,}\PY{n}{valid\PYZus{}targets}\PY{p}{)}
         \PY{n}{train\PYZus{}model}\PY{p}{(}\PY{n}{resnet50\PYZus{}model}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{resnet50}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{train\PYZus{}resnet50}\PY{p}{,}\PY{n}{train\PYZus{}targets}\PY{p}{,}\PY{n}{valid\PYZus{}resnet50}\PY{p}{,}\PY{n}{valid\PYZus{}targets}\PY{p}{)}
         \PY{n}{train\PYZus{}model}\PY{p}{(}\PY{n}{xception\PYZus{}model}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{xception}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{train\PYZus{}xception}\PY{p}{,}\PY{n}{train\PYZus{}targets}\PY{p}{,}\PY{n}{valid\PYZus{}xception}\PY{p}{,}\PY{n}{valid\PYZus{}targets}\PY{p}{)}
         \PY{n}{train\PYZus{}model}\PY{p}{(}\PY{n}{inceptionv3\PYZus{}model}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{inceptionv3}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{train\PYZus{}inceptionv3}\PY{p}{,}\PY{n}{train\PYZus{}targets}\PY{p}{,}\PY{n}{valid\PYZus{}inceptionv3}\PY{p}{,}\PY{n}{valid\PYZus{}targets}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]


Training with VGG19 algorithm
Train on 6680 samples, validate on 835 samples
Epoch 1/20
Epoch 00000: val\_loss improved from inf to 6.51152, saving model to saved\_models/weights.best.vgg19.hdf5
2s - loss: 5.2057 - acc: 0.6760 - val\_loss: 6.5115 - val\_acc: 0.5281
Epoch 2/20
Epoch 00001: val\_loss did not improve
2s - loss: 5.1653 - acc: 0.6751 - val\_loss: 6.5468 - val\_acc: 0.5341
Epoch 3/20
Epoch 00002: val\_loss did not improve
2s - loss: 5.1217 - acc: 0.6795 - val\_loss: 6.5330 - val\_acc: 0.5353
Epoch 4/20
Epoch 00003: val\_loss improved from 6.51152 to 6.47839, saving model to saved\_models/weights.best.vgg19.hdf5
2s - loss: 5.1023 - acc: 0.6813 - val\_loss: 6.4784 - val\_acc: 0.5329
Epoch 5/20
Epoch 00004: val\_loss did not improve
2s - loss: 5.1002 - acc: 0.6819 - val\_loss: 6.5559 - val\_acc: 0.5281
Epoch 6/20
Epoch 00005: val\_loss did not improve
2s - loss: 5.0954 - acc: 0.6819 - val\_loss: 6.5432 - val\_acc: 0.5329
Epoch 7/20
Epoch 00006: val\_loss improved from 6.47839 to 6.44129, saving model to saved\_models/weights.best.vgg19.hdf5
2s - loss: 5.0534 - acc: 0.6829 - val\_loss: 6.4413 - val\_acc: 0.5246
Epoch 8/20
Epoch 00007: val\_loss did not improve
2s - loss: 5.0056 - acc: 0.6859 - val\_loss: 6.5427 - val\_acc: 0.5222
Epoch 9/20
Epoch 00008: val\_loss did not improve
2s - loss: 4.9874 - acc: 0.6883 - val\_loss: 6.4504 - val\_acc: 0.5317
Epoch 10/20
Epoch 00009: val\_loss did not improve
2s - loss: 4.9865 - acc: 0.6894 - val\_loss: 6.4586 - val\_acc: 0.5281
Epoch 11/20
Epoch 00010: val\_loss did not improve
2s - loss: 4.9825 - acc: 0.6895 - val\_loss: 6.4413 - val\_acc: 0.5353
Epoch 12/20
Epoch 00011: val\_loss improved from 6.44129 to 6.33612, saving model to saved\_models/weights.best.vgg19.hdf5
2s - loss: 4.9783 - acc: 0.6898 - val\_loss: 6.3361 - val\_acc: 0.5449
Epoch 13/20
Epoch 00012: val\_loss did not improve
2s - loss: 4.9756 - acc: 0.6901 - val\_loss: 6.4695 - val\_acc: 0.5365
Epoch 14/20
Epoch 00013: val\_loss did not improve
2s - loss: 4.9762 - acc: 0.6904 - val\_loss: 6.4681 - val\_acc: 0.5353
Epoch 15/20
Epoch 00014: val\_loss did not improve
2s - loss: 4.9725 - acc: 0.6904 - val\_loss: 6.3947 - val\_acc: 0.5413
Epoch 16/20
Epoch 00015: val\_loss did not improve
2s - loss: 4.9727 - acc: 0.6906 - val\_loss: 6.4283 - val\_acc: 0.5425
Epoch 17/20
Epoch 00016: val\_loss did not improve
2s - loss: 4.9713 - acc: 0.6910 - val\_loss: 6.4682 - val\_acc: 0.5413
Epoch 18/20
Epoch 00017: val\_loss did not improve
2s - loss: 4.9722 - acc: 0.6910 - val\_loss: 6.4272 - val\_acc: 0.5413
Epoch 19/20
Epoch 00018: val\_loss did not improve
1s - loss: 4.9712 - acc: 0.6912 - val\_loss: 6.4839 - val\_acc: 0.5365
Epoch 20/20
Epoch 00019: val\_loss did not improve
1s - loss: 4.9724 - acc: 0.6909 - val\_loss: 6.4451 - val\_acc: 0.5365


Training with RESNET50 algorithm
Train on 6680 samples, validate on 835 samples
Epoch 1/20
Epoch 00000: val\_loss improved from inf to 0.93380, saving model to saved\_models/weights.best.resnet50.hdf5
2s - loss: 0.0037 - acc: 0.9991 - val\_loss: 0.9338 - val\_acc: 0.8299
Epoch 2/20
Epoch 00001: val\_loss improved from 0.93380 to 0.91712, saving model to saved\_models/weights.best.resnet50.hdf5
2s - loss: 0.0068 - acc: 0.9987 - val\_loss: 0.9171 - val\_acc: 0.8323
Epoch 3/20
Epoch 00002: val\_loss did not improve
2s - loss: 0.0057 - acc: 0.9987 - val\_loss: 0.9436 - val\_acc: 0.8228
Epoch 4/20
Epoch 00003: val\_loss did not improve
2s - loss: 0.0056 - acc: 0.9987 - val\_loss: 0.9789 - val\_acc: 0.8228
Epoch 5/20
Epoch 00004: val\_loss did not improve
2s - loss: 0.0058 - acc: 0.9988 - val\_loss: 0.9930 - val\_acc: 0.8299
Epoch 6/20
Epoch 00005: val\_loss did not improve
2s - loss: 0.0045 - acc: 0.9988 - val\_loss: 0.9875 - val\_acc: 0.8287
Epoch 7/20
Epoch 00006: val\_loss did not improve
2s - loss: 0.0061 - acc: 0.9985 - val\_loss: 1.0421 - val\_acc: 0.8287
Epoch 8/20
Epoch 00007: val\_loss did not improve
2s - loss: 0.0059 - acc: 0.9988 - val\_loss: 0.9998 - val\_acc: 0.8204
Epoch 9/20
Epoch 00008: val\_loss did not improve
2s - loss: 0.0051 - acc: 0.9991 - val\_loss: 1.0603 - val\_acc: 0.8263
Epoch 10/20
Epoch 00009: val\_loss did not improve
2s - loss: 0.0041 - acc: 0.9988 - val\_loss: 1.0677 - val\_acc: 0.8216
Epoch 11/20
Epoch 00010: val\_loss did not improve
2s - loss: 0.0048 - acc: 0.9988 - val\_loss: 1.0539 - val\_acc: 0.8323
Epoch 12/20
Epoch 00011: val\_loss did not improve
2s - loss: 0.0044 - acc: 0.9987 - val\_loss: 1.0718 - val\_acc: 0.8335
Epoch 13/20
Epoch 00012: val\_loss did not improve
2s - loss: 0.0042 - acc: 0.9988 - val\_loss: 1.1337 - val\_acc: 0.8228
Epoch 14/20
Epoch 00013: val\_loss did not improve
2s - loss: 0.0051 - acc: 0.9985 - val\_loss: 1.1466 - val\_acc: 0.8263
Epoch 15/20
Epoch 00014: val\_loss did not improve
2s - loss: 0.0042 - acc: 0.9987 - val\_loss: 1.1326 - val\_acc: 0.8347
Epoch 16/20
Epoch 00015: val\_loss did not improve
2s - loss: 0.0043 - acc: 0.9990 - val\_loss: 1.1300 - val\_acc: 0.8240
Epoch 17/20
Epoch 00016: val\_loss did not improve
2s - loss: 0.0037 - acc: 0.9990 - val\_loss: 1.1442 - val\_acc: 0.8311
Epoch 18/20
Epoch 00017: val\_loss did not improve
2s - loss: 0.0048 - acc: 0.9987 - val\_loss: 1.1433 - val\_acc: 0.8216
Epoch 19/20
Epoch 00018: val\_loss did not improve
2s - loss: 0.0045 - acc: 0.9990 - val\_loss: 1.1844 - val\_acc: 0.8263
Epoch 20/20
Epoch 00019: val\_loss did not improve
2s - loss: 0.0042 - acc: 0.9985 - val\_loss: 1.1728 - val\_acc: 0.8287


Training with XCEPTION algorithm
Train on 6680 samples, validate on 835 samples
Epoch 1/20
Epoch 00000: val\_loss improved from inf to 0.69974, saving model to saved\_models/weights.best.xception.hdf5
5s - loss: 0.0692 - acc: 0.9786 - val\_loss: 0.6997 - val\_acc: 0.8503
Epoch 2/20
Epoch 00001: val\_loss did not improve
5s - loss: 0.0650 - acc: 0.9805 - val\_loss: 0.7319 - val\_acc: 0.8479
Epoch 3/20
Epoch 00002: val\_loss did not improve
6s - loss: 0.0614 - acc: 0.9814 - val\_loss: 0.7625 - val\_acc: 0.8503
Epoch 4/20
Epoch 00003: val\_loss did not improve
6s - loss: 0.0592 - acc: 0.9817 - val\_loss: 0.7255 - val\_acc: 0.8527
Epoch 5/20
Epoch 00004: val\_loss did not improve
6s - loss: 0.0551 - acc: 0.9847 - val\_loss: 0.7625 - val\_acc: 0.8491
Epoch 6/20
Epoch 00005: val\_loss did not improve
6s - loss: 0.0525 - acc: 0.9856 - val\_loss: 0.7724 - val\_acc: 0.8551
Epoch 7/20
Epoch 00006: val\_loss did not improve
6s - loss: 0.0473 - acc: 0.9864 - val\_loss: 0.7931 - val\_acc: 0.8467
Epoch 8/20
Epoch 00007: val\_loss did not improve
5s - loss: 0.0481 - acc: 0.9865 - val\_loss: 0.7926 - val\_acc: 0.8575
Epoch 9/20
Epoch 00008: val\_loss did not improve
6s - loss: 0.0433 - acc: 0.9877 - val\_loss: 0.7645 - val\_acc: 0.8587
Epoch 10/20
Epoch 00009: val\_loss did not improve
5s - loss: 0.0421 - acc: 0.9873 - val\_loss: 0.8008 - val\_acc: 0.8515
Epoch 11/20
Epoch 00010: val\_loss did not improve
5s - loss: 0.0402 - acc: 0.9885 - val\_loss: 0.8198 - val\_acc: 0.8539
Epoch 12/20
Epoch 00011: val\_loss did not improve
5s - loss: 0.0395 - acc: 0.9889 - val\_loss: 0.8631 - val\_acc: 0.8515
Epoch 13/20
Epoch 00012: val\_loss did not improve
6s - loss: 0.0368 - acc: 0.9895 - val\_loss: 0.8216 - val\_acc: 0.8527
Epoch 14/20
Epoch 00013: val\_loss did not improve
5s - loss: 0.0355 - acc: 0.9901 - val\_loss: 0.8712 - val\_acc: 0.8467
Epoch 15/20
Epoch 00014: val\_loss did not improve
5s - loss: 0.0319 - acc: 0.9913 - val\_loss: 0.8483 - val\_acc: 0.8563
Epoch 16/20
Epoch 00015: val\_loss did not improve
5s - loss: 0.0327 - acc: 0.9910 - val\_loss: 0.8539 - val\_acc: 0.8515
Epoch 17/20
Epoch 00016: val\_loss did not improve
5s - loss: 0.0326 - acc: 0.9901 - val\_loss: 0.8323 - val\_acc: 0.8575
Epoch 18/20
Epoch 00017: val\_loss did not improve
5s - loss: 0.0308 - acc: 0.9915 - val\_loss: 0.8890 - val\_acc: 0.8491
Epoch 19/20
Epoch 00018: val\_loss did not improve
5s - loss: 0.0291 - acc: 0.9928 - val\_loss: 0.9133 - val\_acc: 0.8479
Epoch 20/20
Epoch 00019: val\_loss did not improve
5s - loss: 0.0289 - acc: 0.9919 - val\_loss: 0.9287 - val\_acc: 0.8503


Training with INCEPTIONV3 algorithm
Train on 6680 samples, validate on 835 samples
Epoch 1/20
Epoch 00000: val\_loss improved from inf to 0.95906, saving model to saved\_models/weights.best.inceptionv3.hdf5
3s - loss: 0.0258 - acc: 0.9915 - val\_loss: 0.9591 - val\_acc: 0.8491
Epoch 2/20
Epoch 00001: val\_loss did not improve
3s - loss: 0.0261 - acc: 0.9925 - val\_loss: 0.9606 - val\_acc: 0.8503
Epoch 3/20
Epoch 00002: val\_loss did not improve
4s - loss: 0.0226 - acc: 0.9918 - val\_loss: 0.9646 - val\_acc: 0.8539
Epoch 4/20
Epoch 00003: val\_loss did not improve
3s - loss: 0.0216 - acc: 0.9934 - val\_loss: 1.0357 - val\_acc: 0.8479
Epoch 5/20
Epoch 00004: val\_loss did not improve
3s - loss: 0.0186 - acc: 0.9940 - val\_loss: 1.0093 - val\_acc: 0.8515
Epoch 6/20
Epoch 00005: val\_loss did not improve
4s - loss: 0.0188 - acc: 0.9946 - val\_loss: 1.0192 - val\_acc: 0.8503
Epoch 7/20
Epoch 00006: val\_loss did not improve
4s - loss: 0.0198 - acc: 0.9946 - val\_loss: 1.0109 - val\_acc: 0.8491
Epoch 8/20
Epoch 00007: val\_loss did not improve
4s - loss: 0.0160 - acc: 0.9958 - val\_loss: 1.0641 - val\_acc: 0.8563
Epoch 9/20
Epoch 00008: val\_loss did not improve
4s - loss: 0.0126 - acc: 0.9969 - val\_loss: 1.0627 - val\_acc: 0.8551
Epoch 10/20
Epoch 00009: val\_loss did not improve
3s - loss: 0.0122 - acc: 0.9964 - val\_loss: 1.0867 - val\_acc: 0.8467
Epoch 11/20
Epoch 00010: val\_loss did not improve
4s - loss: 0.0137 - acc: 0.9963 - val\_loss: 1.0327 - val\_acc: 0.8515
Epoch 12/20
Epoch 00011: val\_loss did not improve
4s - loss: 0.0111 - acc: 0.9966 - val\_loss: 1.0784 - val\_acc: 0.8551
Epoch 13/20
Epoch 00012: val\_loss did not improve
4s - loss: 0.0103 - acc: 0.9970 - val\_loss: 1.0784 - val\_acc: 0.8551
Epoch 14/20
Epoch 00013: val\_loss did not improve
4s - loss: 0.0094 - acc: 0.9969 - val\_loss: 1.1380 - val\_acc: 0.8563
Epoch 15/20
Epoch 00014: val\_loss did not improve
4s - loss: 0.0090 - acc: 0.9976 - val\_loss: 1.1249 - val\_acc: 0.8527
Epoch 16/20
Epoch 00015: val\_loss did not improve
4s - loss: 0.0093 - acc: 0.9975 - val\_loss: 1.1086 - val\_acc: 0.8479
Epoch 17/20
Epoch 00016: val\_loss did not improve
4s - loss: 0.0096 - acc: 0.9975 - val\_loss: 1.1531 - val\_acc: 0.8563
Epoch 18/20
Epoch 00017: val\_loss did not improve
4s - loss: 0.0064 - acc: 0.9978 - val\_loss: 1.1519 - val\_acc: 0.8635
Epoch 19/20
Epoch 00018: val\_loss did not improve
4s - loss: 0.0069 - acc: 0.9981 - val\_loss: 1.1259 - val\_acc: 0.8527
Epoch 20/20
Epoch 00019: val\_loss did not improve
4s - loss: 0.0084 - acc: 0.9978 - val\_loss: 1.1667 - val\_acc: 0.8515

    \end{Verbatim}

    \hypertarget{implementation-load-the-model-with-the-best-validation-loss}{%
\subsubsection{(IMPLEMENTATION) Load the Model with the Best Validation
Loss}\label{implementation-load-the-model-with-the-best-validation-loss}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}41}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} TODO: Load the model weights with the best validation loss.}
         \PY{k}{def} \PY{n+nf}{load\PYZus{}model\PYZus{}weights}\PY{p}{(}\PY{n}{modeo}\PY{p}{,}\PY{n}{checkpoint\PYZus{}file}\PY{p}{)}\PY{p}{:}
             \PY{n}{modeo}\PY{o}{.}\PY{n}{load\PYZus{}weights}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{saved\PYZus{}models/weights.best.}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{.hdf5}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{checkpoint\PYZus{}file}\PY{p}{)}\PY{p}{)}
             
         \PY{n}{load\PYZus{}model\PYZus{}weights}\PY{p}{(}\PY{n}{vgg19\PYZus{}model}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{vgg19}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{load\PYZus{}model\PYZus{}weights}\PY{p}{(}\PY{n}{resnet50\PYZus{}model}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{resnet50}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{load\PYZus{}model\PYZus{}weights}\PY{p}{(}\PY{n}{xception\PYZus{}model}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{xception}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{load\PYZus{}model\PYZus{}weights}\PY{p}{(}\PY{n}{inceptionv3\PYZus{}model}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{inceptionv3}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \hypertarget{implementation-test-the-model}{%
\subsubsection{(IMPLEMENTATION) Test the
Model}\label{implementation-test-the-model}}

Try out your model on the test dataset of dog images. Ensure that your
test accuracy is greater than 60\%.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}46}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} TODO: Calculate classification accuracy on the test dataset.}
         \PY{k}{def} \PY{n+nf}{test\PYZus{}model}\PY{p}{(}\PY{n}{modeo}\PY{p}{,}\PY{n}{test\PYZus{}tensors}\PY{p}{,}\PY{n}{name}\PY{p}{)}\PY{p}{:}
             \PY{k}{global} \PY{n}{test\PYZus{}targets}
             \PY{n}{preds} \PY{o}{=} \PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{modeo}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{expand\PYZus{}dims}\PY{p}{(}\PY{n}{feature}\PY{p}{,}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{)}\PY{p}{)} \PY{k}{for} \PY{n}{feature} \PY{o+ow}{in} \PY{n}{test\PYZus{}tensors}\PY{p}{]}
             \PY{n}{test\PYZus{}acc} \PY{o}{=} \PY{l+m+mi}{100}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{preds}\PY{p}{)}\PY{o}{==}\PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{test\PYZus{}targets}\PY{p}{,}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{o}{/}\PY{n+nb}{len}\PY{p}{(}\PY{n}{preds}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{ accuracy is }\PY{l+s+si}{\PYZob{}:.4f\PYZcb{}}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{name}\PY{o}{.}\PY{n}{upper}\PY{p}{(}\PY{p}{)}\PY{p}{,}\PY{n}{test\PYZus{}acc}\PY{p}{)}\PY{p}{)}
             
         \PY{n}{test\PYZus{}model}\PY{p}{(}\PY{n}{vgg19\PYZus{}model}\PY{p}{,}\PY{n}{test\PYZus{}vgg19}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{vgg19}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{test\PYZus{}model}\PY{p}{(}\PY{n}{resnet50\PYZus{}model}\PY{p}{,}\PY{n}{test\PYZus{}resnet50}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{resnet50}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{test\PYZus{}model}\PY{p}{(}\PY{n}{xception\PYZus{}model}\PY{p}{,}\PY{n}{test\PYZus{}xception}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{xception}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{test\PYZus{}model}\PY{p}{(}\PY{n}{inceptionv3\PYZus{}model}\PY{p}{,}\PY{n}{test\PYZus{}inceptionv3}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{inceptionv3}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
VGG19 accuracy is 53.5885\%
RESNET50 accuracy is 81.4593\%
XCEPTION accuracy is 85.8852\%
INCEPTIONV3 accuracy is 83.0144\%

    \end{Verbatim}

    \hypertarget{implementation-predict-dog-breed-with-the-model}{%
\subsubsection{(IMPLEMENTATION) Predict Dog Breed with the
Model}\label{implementation-predict-dog-breed-with-the-model}}

Write a function that takes an image path as input and returns the dog
breed (\texttt{Affenpinscher}, \texttt{Afghan\_hound}, etc) that is
predicted by your model.

Similar to the analogous function in Step 5, your function should have
three steps: 1. Extract the bottleneck features corresponding to the
chosen CNN model. 2. Supply the bottleneck features as input to the
model to return the predicted vector. Note that the argmax of this
prediction vector gives the index of the predicted dog breed. 3. Use the
\texttt{dog\_names} array defined in Step 0 of this notebook to return
the corresponding breed.

The functions to extract the bottleneck features can be found in
\texttt{extract\_bottleneck\_features.py}, and they have been imported
in an earlier code cell. To obtain the bottleneck features corresponding
to your chosen CNN architecture, you need to use the function

\begin{verbatim}
extract_{network}
\end{verbatim}

where \texttt{\{network\}}, in the above filename, should be one of
\texttt{VGG19}, \texttt{Resnet50}, \texttt{InceptionV3}, or
\texttt{Xception}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}50}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} TODO: Write a function that takes a path to an image as input}
         \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} and returns the dog breed that is predicted by the model.}
         
         \PY{k+kn}{from} \PY{n+nn}{extract\PYZus{}bottleneck\PYZus{}features} \PY{k}{import} \PY{o}{*}
         \PY{k}{def} \PY{n+nf}{predict\PYZus{}dog\PYZus{}breed}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{,}\PY{n}{network}\PY{p}{)}\PY{p}{:}
             \PY{c+c1}{\PYZsh{}bottleneck\PYZus{}feats = extract\PYZus{}Xception(path\PYZus{}to\PYZus{}tensor(img\PYZus{}path))}
             \PY{n}{bottleneck\PYZus{}feats} \PY{o}{=} \PY{n+nb}{eval}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{extract\PYZus{}}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{(path\PYZus{}to\PYZus{}tensor(img\PYZus{}path))}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{network}\PY{p}{)}\PY{p}{)}
             \PY{n}{pred} \PY{o}{=} \PY{n}{xception\PYZus{}model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{bottleneck\PYZus{}feats}\PY{p}{)}
             \PY{k}{return} \PY{n}{dog\PYZus{}names}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{pred}\PY{p}{)}\PY{p}{]}
         
         \PY{n}{predicted\PYZus{}dog} \PY{o}{=} \PY{n}{predict\PYZus{}dog\PYZus{}breed}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{images/Labrador\PYZus{}retriever\PYZus{}06457.jpg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Xception}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{The predicted dog is: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{predicted\PYZus{}dog}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
The predicted dog is: Labrador\_retriever

    \end{Verbatim}

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

 \#\# Step 6: Write your Algorithm

Write an algorithm that accepts a file path to an image and first
determines whether the image contains a human, dog, or neither. Then, -
if a \textbf{dog} is detected in the image, return the predicted breed.
- if a \textbf{human} is detected in the image, return the resembling
dog breed. - if \textbf{neither} is detected in the image, provide
output that indicates an error.

You are welcome to write your own functions for detecting humans and
dogs in images, but feel free to use the \texttt{face\_detector} and
\texttt{dog\_detector} functions developed above. You are
\textbf{required} to use your CNN from Step 5 to predict dog breed.

Some sample output for our algorithm is provided below, but feel free to
design your own user experience!

\begin{figure}
\centering
\includegraphics{images/sample_human_output.png}
\caption{Sample Human Output}
\end{figure}

\hypertarget{implementation-write-your-algorithm}{%
\subsubsection{(IMPLEMENTATION) Write your
Algorithm}\label{implementation-write-your-algorithm}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}85}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} TODO: Write your algorithm.}
         \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} Feel free to use as many code cells as needed.}
         \PY{k}{def} \PY{n+nf}{whats\PYZus{}the\PYZus{}dog\PYZus{}breed}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}\PY{p}{:}
             \PY{n}{is\PYZus{}dog} \PY{o}{=}  \PY{n}{dog\PYZus{}detector}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}\PY{p}{;}
             \PY{n}{tensor} \PY{o}{=} \PY{n}{path\PYZus{}to\PYZus{}tensor}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}
             \PY{n}{pred\PYZus{}dog} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{tensor}\PY{p}{)}
             \PY{n}{dog\PYZus{}name} \PY{o}{=} \PY{n}{dog\PYZus{}names}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{pred\PYZus{}dog}\PY{p}{)}\PY{p}{]}
             \PY{k}{if}\PY{p}{(}\PY{n}{is\PYZus{}dog}\PY{p}{)}\PY{p}{:}    
                 \PY{k}{return} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{I think the dog breed for }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{ is: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{,}\PY{n}{dog\PYZus{}name}\PY{p}{)}
             \PY{k}{elif} \PY{n}{face\PYZus{}detector}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}\PY{p}{:}
                 \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{I think }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{ is not a dog, but a human being}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{img\PYZus{}path}\PY{p}{)}\PY{p}{,}\PY{n}{end}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ }\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}     
                 \PY{k}{return} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{But, the human provided looks like a }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{ :D}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{dog\PYZus{}name}\PY{p}{)}
             \PY{k}{else}\PY{p}{:}
                 \PY{k}{return} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Sorry, but I don}\PY{l+s+se}{\PYZbs{}\PYZsq{}}\PY{l+s+s1}{t know what type of creature you}\PY{l+s+se}{\PYZbs{}\PYZsq{}}\PY{l+s+s1}{ve provided to predict}\PY{l+s+s1}{\PYZsq{}}
             
         
         \PY{n}{test\PYZus{}imgs} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{images/Labrador\PYZus{}retriever\PYZus{}06457.jpg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                      \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{images/sample\PYZus{}human\PYZus{}output.png}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                     \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{images/sample\PYZus{}cnn.png}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
         \PY{k}{for} \PY{n}{img} \PY{o+ow}{in} \PY{n}{test\PYZus{}imgs}\PY{p}{:}
            \PY{n+nb}{print}\PY{p}{(}\PY{n}{whats\PYZus{}the\PYZus{}dog\PYZus{}breed}\PY{p}{(}\PY{n}{img}\PY{p}{)}\PY{p}{)}
         
             
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
I think the dog breed for images/Labrador\_retriever\_06457.jpg is: English\_springer\_spaniel
I think images/sample\_human\_output.png is not a dog, but a human being But, the human provided looks like a Dalmatian :D
Sorry, but I don't know what type of creature you've provided to predict

    \end{Verbatim}

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

 \#\# Step 7: Test Your Algorithm

In this section, you will take your new algorithm for a spin! What kind
of dog does the algorithm think that \textbf{you} look like? If you have
a dog, does it predict your dog's breed accurately? If you have a cat,
does it mistakenly think that your cat is a dog?

\hypertarget{implementation-test-your-algorithm-on-sample-images}{%
\subsubsection{(IMPLEMENTATION) Test Your Algorithm on Sample
Images!}\label{implementation-test-your-algorithm-on-sample-images}}

Test your algorithm at least six images on your computer. Feel free to
use any images you like. Use at least two human and two dog images.

\textbf{Question 6:} Is the output better than you expected :) ? Or
worse :( ? Provide at least three possible points of improvement for
your algorithm.

\textbf{Answer:} I think with 23\% acc it's quite good performance. Of
course it could do better. I think I can improve algorithm acc
implementing:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  More layers. I think my model suffered because its so shallow
  regarding number of layers, maybe if I stack 2 more I'll get at least
  40\% with 100 epochs;
\item
  I wanted to try the
  \href{https://github.com/abgoswam/deepdish}{deepdish approach}, I
  think using 3*Convs(5x5)-\textgreater{}(Conv-\textgreater{}MaxPool)*2
  with some tuning my model would improve more;
\item
  My lack of GPU damaged my performance. Of course I could do better, my
  it took 12h to complete training with this architecture;
\item
  More studying, I'm barely scratching the surface of CNN's. I took a
  look in a couple of papers I'm I realized I'm very far from good right
  now.
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}101}]:} \PY{c+c1}{\PYZsh{}\PYZsh{} TODO: Execute your algorithm from Step 6 on}
          \PY{c+c1}{\PYZsh{}\PYZsh{} at least 6 images on your computer.}
          \PY{c+c1}{\PYZsh{}\PYZsh{} Feel free to use as many code cells as needed.}
          \PY{k+kn}{import} \PY{n+nn}{urllib}\PY{n+nn}{.}\PY{n+nn}{request}
          \PY{k+kn}{from} \PY{n+nn}{pathlib} \PY{k}{import} \PY{n}{Path}
          
          \PY{n}{images\PYZus{}} \PY{o}{=} \PY{p}{\PYZob{}}
              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{a\PYZus{}dog.jpg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{https://www.cesarsway.com/sites/newcesarsway/files/styles/large\PYZus{}article\PYZus{}preview/public/Common\PYZhy{}dog\PYZhy{}behaviors\PYZhy{}explained.jpg?itok=FSzwbBoi}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{a\PYZus{}cat.jpg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{http://a57.foxnews.com/images.foxnews.com/content/fox\PYZhy{}news/lifestyle/2017/11/09/how\PYZhy{}to\PYZhy{}keep\PYZhy{}cat\PYZhy{}from\PYZhy{}scratching\PYZhy{}your\PYZhy{}sofa\PYZhy{}to\PYZhy{}shreds/\PYZus{}jcr\PYZus{}content/par/featured\PYZus{}image/media\PYZhy{}0.img.jpg/931/524/1510172827500.jpg?ve=1\PYZam{}tl=1\PYZam{}text=big\PYZhy{}top\PYZhy{}image}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{homer.jpg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{https://vignette.wikia.nocookie.net/simpsons/images/b/b4/Homer\PYZus{}drool.jpg/revision/latest?cb=20100923210150}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{grim\PYZus{}reaper.png}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{https://pm1.narvii.com/6579/20c0195934757df8c741ba541ee774f43490115c\PYZus{}hq.jpg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{inoue.jpg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{https://www.absoluteanime.com/bleach/orihime.jpg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{kenshin.jpg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{https://jovemnerd.com.br/wp\PYZhy{}content/uploads/RK071208.jpg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{3\PYZus{}golden\PYZus{}retriever.jpg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{https://vetstreet.brightspotcdn.com/dims4/default/153b621/2147483647/thumbnail/645x380/quality/90/?url=https}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{3A}\PY{l+s+si}{\PYZpc{}2F}\PY{l+s+si}{\PYZpc{}2F}\PY{l+s+s1}{vetstreet\PYZhy{}brightspot.s3.amazonaws.com}\PY{l+s+si}{\PYZpc{}2F}\PY{l+s+s1}{51}\PY{l+s+si}{\PYZpc{}2F}\PY{l+s+s1}{3d}\PY{l+s+si}{\PYZpc{}2F}\PY{l+s+s1}{7a22f69c46f587a6730c36b2c2b1}\PY{l+s+si}{\PYZpc{}2F}\PY{l+s+s1}{golden\PYZhy{}retriever\PYZhy{}ap\PYZhy{}zfje1k\PYZhy{}645.jpg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mini\PYZus{}schnauzer}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{https://img.elo7.com.br/product/original/1938E58/miniatura\PYZhy{}schnauzer\PYZhy{}em\PYZhy{}resina\PYZhy{}schnauzer.jpg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{muttley.jpg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{https://images\PYZhy{}na.ssl\PYZhy{}images\PYZhy{}amazon.com/images/I/41m0u\PYZhy{}4yBwL.\PYZus{}SY355\PYZus{}.jpg}\PY{l+s+s1}{\PYZsq{}}
          \PY{p}{\PYZcb{}}
          
          \PY{k}{def} \PY{n+nf}{download\PYZus{}images}\PY{p}{(}\PY{n}{images\PYZus{}}\PY{p}{)}\PY{p}{:}
              \PY{k}{for} \PY{n}{name}\PY{p}{,}\PY{n}{url} \PY{o+ow}{in} \PY{n}{images\PYZus{}}\PY{o}{.}\PY{n}{items}\PY{p}{(}\PY{p}{)}\PY{p}{:}     
                  \PY{n}{file} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{images/}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n}{name}
                  \PY{n}{lbp} \PY{o}{=} \PY{n}{Path}\PY{p}{(}\PY{n}{file}\PY{p}{)}
                  \PY{k}{if} \PY{o+ow}{not} \PY{n}{lbp}\PY{o}{.}\PY{n}{is\PYZus{}file}\PY{p}{(}\PY{p}{)}\PY{p}{:}
                      \PY{c+c1}{\PYZsh{}print(\PYZsq{}Downloading \PYZob{}\PYZcb{}\PYZsq{}.format(name))}
                      \PY{n}{urllib}\PY{o}{.}\PY{n}{request}\PY{o}{.}\PY{n}{urlretrieve}\PY{p}{(}\PY{n}{url}\PY{p}{,} \PY{n}{file}\PY{p}{)} 
                      
          \PY{n}{download\PYZus{}images}\PY{p}{(}\PY{n}{images\PYZus{}}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{}Finally running the test}
          \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{Trying to predict correctly, but with 23}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{ acc :}\PY{l+s+se}{\PYZbs{}\PYZsq{}}\PY{l+s+s1}{(}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          \PY{k}{for} \PY{n}{name}\PY{p}{,}\PY{n}{\PYZus{}} \PY{o+ow}{in} \PY{n}{images\PYZus{}}\PY{o}{.}\PY{n}{items}\PY{p}{(}\PY{p}{)}\PY{p}{:}
              \PY{n}{file} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{images/}\PY{l+s+s1}{\PYZsq{}}\PY{o}{+}\PY{n}{name}
              \PY{n+nb}{print}\PY{p}{(}\PY{n}{whats\PYZus{}the\PYZus{}dog\PYZus{}breed}\PY{p}{(}\PY{n}{file}\PY{p}{)}\PY{p}{)}
              
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]


Trying to predict correctly, but with 23\% acc :'(
I think the dog breed for images/a\_dog.jpg is: English\_springer\_spaniel
Sorry, but I don't know what type of creature you've provided to predict
Sorry, but I don't know what type of creature you've provided to predict
Sorry, but I don't know what type of creature you've provided to predict
Sorry, but I don't know what type of creature you've provided to predict
I think images/kenshin.jpg is not a dog, but a human being But, the human provided looks like a English\_springer\_spaniel :D
I think the dog breed for images/3\_golden\_retriever.jpg is: Dalmatian
I think the dog breed for images/mini\_schnauzer is: Black\_russian\_terrier
Sorry, but I don't know what type of creature you've provided to predict

    \end{Verbatim}


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
